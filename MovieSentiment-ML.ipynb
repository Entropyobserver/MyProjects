{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Project Overview: **MovieSentiment-ML**\n",
    "\n",
    "This project demonstrates **sentiment analysis** using the Cornell Movie Review dataset (2000 reviews) with sklearn implementations.\n",
    "\n",
    "### Key Components:\n",
    "\n",
    "**1. Dataset Parser**\n",
    "- Loads positive/negative movie reviews\n",
    "- Returns raw text and labels {-1, +1}\n",
    "\n",
    "**2. Feature Engineering**\n",
    "- **BOW (Bag of Words)**: Binary word presence/absence\n",
    "- **TF-IDF**: Term frequency × inverse document frequency weighting\n",
    "\n",
    "**3. Optimization Methods**\n",
    "- **SGD-Optimal**: Auto-tuned learning rate\n",
    "- **SGD-Adaptive**: Learning rate decay\n",
    "- **SVM-SMO**: Sequential Minimal Optimization\n",
    "- **SVM-RBF**: Radial Basis Function kernel\n",
    "\n",
    "**4. Evaluation**\n",
    "- **5-fold cross-validation** for robust performance estimation\n",
    "- Comparison across feature × optimizer combinations\n",
    "\n",
    "### Results Analysis:\n",
    "The experiment compares different combinations to find the best performing setup for movie review sentiment classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Parsing the dataset\n",
    "\n",
    "**Implementation task:** Implement a parser for the dataset. The output should be a list/array of strings (`X_raw`) and a list/array of labels (`y`) encoded as {-1,1}.\n",
    "\n",
    "Dataset Structure: Review Polarity v2.0\n",
    "\n",
    "Positive Reviews: 1000\n",
    "\n",
    "Negative Reviews: 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 2000\n",
      "Positive samples: 1000\n",
      "Negative samples: 1000\n",
      "films adapted from comic books have had plenty of success , whether they're about superheroes ( batman , superman , spawn ) , or geared toward kids ( casper ) or the arthouse crowd ( ghost world ) , but there's never really been a comic book like from hell before . \n",
      "for starters , it was created by alan moore ( and eddie campbell ) , who brought the medium to a whole new level in the mid '80s with a 12-part series called the watchmen . \n",
      "to say moore and campbell thoroughly researched the subject of jack the ripper would be like saying michael jackson is starting to look a little odd . \n",
      "the book ( or \" graphic novel , \" if you will ) is over 500 pages long and includes nearly 30 more that consist of nothing but footnotes . \n",
      "in other words , don't dismiss this film because of its source . \n",
      "if you can get past the whole comic book thing , you might find another stumbling block in from hell's directors , albert and allen hughes . \n",
      "getting the hughes brothers to direct this seems almost as ludicrous as casting carrot top in , well , anything , but riddle me this : who better to direct a film that's set in the ghetto and features really violent street crime than the mad geniuses behind menace ii society ? \n",
      "the ghetto in question is , of course , whitechapel in 1888 london's east end . \n",
      "it's a filthy , sooty place where the whores ( called \" unfortunates \" ) are starting to get a little nervous about this mysterious psychopath who has been carving through their profession with surgical precision . \n",
      "when the first stiff turns up , copper peter godley ( robbie coltrane , the world is not enough ) calls in inspector frederick abberline ( johnny depp , blow ) to crack the case . \n",
      "abberline , a widower , has prophetic dreams he unsuccessfully tries to quell with copious amounts of absinthe and opium . \n",
      "upon arriving in whitechapel , he befriends an unfortunate named mary kelly ( heather graham , say it isn't so ) and proceeds to investigate the horribly gruesome crimes that even the police surgeon can't stomach . \n",
      "i don't think anyone needs to be briefed on jack the ripper , so i won't go into the particulars here , other than to say moore and campbell have a unique and interesting theory about both the identity of the killer and the reasons he chooses to slay . \n",
      "in the comic , they don't bother cloaking the identity of the ripper , but screenwriters terry hayes ( vertical limit ) and rafael yglesias ( les mis ? rables ) do a good job of keeping him hidden from viewers until the very end . \n",
      "it's funny to watch the locals blindly point the finger of blame at jews and indians because , after all , an englishman could never be capable of committing such ghastly acts . \n",
      "and from hell's ending had me whistling the stonecutters song from the simpsons for days ( \" who holds back the electric car/who made steve guttenberg a star ? \" ) . \n",
      "don't worry - it'll all make sense when you see it . \n",
      "now onto from hell's appearance : it's certainly dark and bleak enough , and it's surprising to see how much more it looks like a tim burton film than planet of the apes did ( at times , it seems like sleepy hollow 2 ) . \n",
      "the print i saw wasn't completely finished ( both color and music had not been finalized , so no comments about marilyn manson ) , but cinematographer peter deming ( don't say a word ) ably captures the dreariness of victorian-era london and helped make the flashy killing scenes remind me of the crazy flashbacks in twin peaks , even though the violence in the film pales in comparison to that in the black-and-white comic . \n",
      "oscar winner martin childs' ( shakespeare in love ) production design turns the original prague surroundings into one creepy place . \n",
      "even the acting in from hell is solid , with the dreamy depp turning in a typically strong performance and deftly handling a british accent . \n",
      "ians holm ( joe gould's secret ) and richardson ( 102 dalmatians ) log in great supporting roles , but the big surprise here is graham . \n",
      "i cringed the first time she opened her mouth , imagining her attempt at an irish accent , but it actually wasn't half bad . \n",
      "the film , however , is all good . \n",
      "2 : 00 - r for strong violence/gore , sexuality , language and drug content\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def parser(dataset_path='./txt_sentoken'):\n",
    "    X_raw, y = [], []\n",
    "    \n",
    "    pos_folder_path = os.path.join(dataset_path, 'pos')\n",
    "    for filename in os.listdir(pos_folder_path):\n",
    "        file_path = os.path.join(pos_folder_path, filename)\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\", errors='ignore') as file:\n",
    "            X_raw.append(file.read().strip().lower())\n",
    "        y.append(1)\n",
    "    \n",
    "\n",
    "    neg_folder_path = os.path.join(dataset_path, 'neg')\n",
    "    for filename in os.listdir(neg_folder_path):\n",
    "        file_path = os.path.join(neg_folder_path, filename)\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\", errors='ignore') as file:\n",
    "            X_raw.append(file.read().strip().lower())\n",
    "        y.append(-1)\n",
    "    \n",
    "    y = np.array(y)\n",
    "\n",
    "    print(f\"Total samples: {len(X_raw)}\")\n",
    "    print(f\"Positive samples: {sum(y == 1)}\")\n",
    "    print(f\"Negative samples: {sum(y == -1)}\")\n",
    "\n",
    "    return X_raw, y\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    X_raw, y = parser()\n",
    "    print(X_raw[0])  \n",
    "    print(y[0])  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Feature extraction\n",
    "\n",
    "\n",
    "Why Use Binary Bag-of-Words (BOW)?\n",
    "\n",
    "Advantages:\n",
    "\n",
    "Reduces the impact of high-frequency words: For example, if \"movie\" appears 100 times, its influence won't overshadow less frequent but meaningful words like \"great.\"\n",
    "\n",
    "Well-suited for classification tasks: Focuses on the presence of words rather than their frequency.\n",
    "\n",
    "Minimizes the effect of common words: Words like \"the,\" \"is,\" and \"and\" won't disproportionately affect the model's performance.\n",
    "\n",
    "Disadvantages:\n",
    "\n",
    "Loses frequency information: This can reduce accuracy for tasks like topic modeling or information retrieval that rely on word frequency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 2000\n",
      "Positive samples: 1000\n",
      "Negative samples: 1000\n",
      "Sparse Matrix Shape (Sklearn): (2000, 39659)\n",
      "First sentence vector (Sklearn, Sparse):   (0, 13196)\t1\n",
      "  (0, 1014)\t1\n",
      "  (0, 14073)\t8\n",
      "  (0, 7039)\t5\n",
      "  (0, 4366)\t1\n",
      "  (0, 16028)\t2\n",
      "  (0, 15656)\t3\n",
      "  (0, 26439)\t1\n",
      "  (0, 24386)\t14\n",
      "  (0, 34108)\t1\n",
      "  (0, 38707)\t1\n",
      "  (0, 35351)\t2\n",
      "  (0, 28303)\t1\n",
      "  (0, 750)\t4\n",
      "  (0, 34291)\t1\n",
      "  (0, 3308)\t1\n",
      "  (0, 34299)\t1\n",
      "  (0, 32906)\t1\n",
      "  (0, 24635)\t3\n",
      "  (0, 14473)\t1\n",
      "  (0, 35949)\t1\n",
      "  (0, 19446)\t1\n",
      "  (0, 5688)\t1\n",
      "  (0, 35280)\t46\n",
      "  (0, 2359)\t1\n",
      "  :\t:\n",
      "  (0, 20760)\t1\n",
      "  (0, 15246)\t1\n",
      "  (0, 34350)\t1\n",
      "  (0, 29846)\t1\n",
      "  (0, 3820)\t1\n",
      "  (0, 34399)\t1\n",
      "  (0, 8266)\t1\n",
      "  (0, 35611)\t1\n",
      "  (0, 31471)\t1\n",
      "  (0, 24559)\t1\n",
      "  (0, 16315)\t2\n",
      "  (0, 23100)\t1\n",
      "  (0, 17382)\t1\n",
      "  (0, 2628)\t1\n",
      "  (0, 18535)\t1\n",
      "  (0, 992)\t1\n",
      "  (0, 15706)\t1\n",
      "  (0, 2954)\t1\n",
      "  (0, 16926)\t1\n",
      "  (0, 0)\t1\n",
      "  (0, 15055)\t1\n",
      "  (0, 31327)\t1\n",
      "  (0, 19929)\t1\n",
      "  (0, 10792)\t1\n",
      "  (0, 7632)\t1\n",
      "\n",
      "First sentence vectorized using dense matrix (Sklearn):\n",
      " [[1 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer as SklearnCountVectorizer\n",
    "if __name__ == \"__main__\":\n",
    "    X_raw, y = parser() \n",
    "    sklearn_countvec= SklearnCountVectorizer()\n",
    "    X_sklearn = sklearn_countvec.fit_transform(X_raw)\n",
    "    print(\"Sparse Matrix Shape (Sklearn):\", X_sklearn.shape)\n",
    "    first_sentence_sklearn_vector = X_sklearn[0]\n",
    "    print(\"First sentence vector (Sklearn, Sparse):\", first_sentence_sklearn_vector)\n",
    "    first_sentence_dense_sklearn = first_sentence_sklearn_vector.toarray()\n",
    "    print(\"\\nFirst sentence vectorized using dense matrix (Sklearn):\\n\", first_sentence_dense_sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 2000\n",
      "Positive samples: 1000\n",
      "Negative samples: 1000\n",
      "BOW accuracy: 0.8200\n",
      "TF-IDF accuracy: 0.8175\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def train_and_test(vectorizer, X_train, X_test, y_train, y_test):\n",
    "    X_train_vec = vectorizer.fit_transform(X_train)\n",
    "    X_test_vec = vectorizer.transform(X_test)\n",
    "    \n",
    "    model = SGDClassifier(loss='hinge', random_state=42)\n",
    "    model.fit(X_train_vec, y_train)\n",
    "    \n",
    "    y_pred = model.predict(X_test_vec)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    return acc\n",
    "\n",
    "def sklearn_baseline():\n",
    "    X_raw, y = parser()\n",
    "    y = (y + 1) // 2 \n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_raw, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "\n",
    "    bow_acc = train_and_test(CountVectorizer(), X_train, X_test, y_train, y_test)\n",
    "    tfidf_acc = train_and_test(TfidfVectorizer(), X_train, X_test, y_train, y_test)\n",
    "    \n",
    "    print(f\"BOW accuracy: {bow_acc:.4f}\")\n",
    "    print(f\"TF-IDF accuracy: {tfidf_acc:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sklearn_baseline()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Learning framework\n",
    "\n",
    "The main goal is to implement these components (the model, the loss function, and gradient descent) and iteratively train the model until it converges.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== DETAILED EVALUATION =====\n",
      "\n",
      "BOW Results:\n",
      "Accuracy:  0.8275\n",
      "Precision: 0.8385\n",
      "Recall:    0.8090\n",
      "F1 Score:  0.8235\n",
      "CV Mean:   0.8425 ± 0.0131\n",
      "\n",
      "TF-IDF Results:\n",
      "Accuracy:  0.8250\n",
      "Precision: 0.8146\n",
      "Recall:    0.8392\n",
      "F1 Score:  0.8267\n",
      "CV Mean:   0.8535 ± 0.0041\n",
      "\n",
      "Confusion Matrix - BOW:\n",
      "[[170  31]\n",
      " [ 38 161]]\n",
      "\n",
      "Confusion Matrix - TF-IDF:\n",
      "[[163  38]\n",
      " [ 32 167]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def parser(dataset_path='./txt_sentoken'):\n",
    "    X_raw, y = [], []\n",
    "    pos_folder = os.path.join(dataset_path, 'pos')\n",
    "    for filename in os.listdir(pos_folder):\n",
    "        with open(os.path.join(pos_folder, filename), \"r\", encoding=\"utf-8\", errors='ignore') as file:\n",
    "            X_raw.append(file.read().strip().lower())\n",
    "        y.append(1)\n",
    "    neg_folder = os.path.join(dataset_path, 'neg')\n",
    "    for filename in os.listdir(neg_folder):\n",
    "        with open(os.path.join(neg_folder, filename), \"r\", encoding=\"utf-8\", errors='ignore') as file:\n",
    "            X_raw.append(file.read().strip().lower())\n",
    "        y.append(-1)\n",
    "    return X_raw, np.array(y)\n",
    "\n",
    "def sklearn_implementation():\n",
    "    X_raw, y = parser()\n",
    "    y_binary = (y + 1) // 2\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_raw, y_binary, test_size=0.2, random_state=42)\n",
    "\n",
    "    bow_pipeline = make_pipeline(\n",
    "        CountVectorizer(binary=True, max_features=10000),\n",
    "        SGDClassifier(loss='hinge', random_state=42)\n",
    "    )\n",
    "    tfidf_pipeline = make_pipeline(\n",
    "        TfidfVectorizer(max_features=10000),\n",
    "        SGDClassifier(loss='hinge', random_state=42)\n",
    "    )\n",
    "\n",
    "    bow_pipeline.fit(X_train, y_train)\n",
    "    tfidf_pipeline.fit(X_train, y_train)\n",
    "    bow_pred = bow_pipeline.predict(X_test)\n",
    "    tfidf_pred = tfidf_pipeline.predict(X_test)\n",
    "\n",
    "    cv_bow_scores = cross_val_score(bow_pipeline, X_raw, y_binary, cv=5)\n",
    "    cv_tfidf_scores = cross_val_score(tfidf_pipeline, X_raw, y_binary, cv=5)\n",
    "\n",
    "    print(\"===== DETAILED EVALUATION =====\")\n",
    "    print(\"\\nBOW Results:\")\n",
    "    print(f\"Accuracy:  {accuracy_score(y_test, bow_pred):.4f}\")\n",
    "    print(f\"Precision: {precision_score(y_test, bow_pred):.4f}\")\n",
    "    print(f\"Recall:    {recall_score(y_test, bow_pred):.4f}\")\n",
    "    print(f\"F1 Score:  {f1_score(y_test, bow_pred):.4f}\")\n",
    "    print(f\"CV Mean:   {cv_bow_scores.mean():.4f} ± {cv_bow_scores.std():.4f}\")\n",
    "\n",
    "    print(\"\\nTF-IDF Results:\")\n",
    "    print(f\"Accuracy:  {accuracy_score(y_test, tfidf_pred):.4f}\")\n",
    "    print(f\"Precision: {precision_score(y_test, tfidf_pred):.4f}\")\n",
    "    print(f\"Recall:    {recall_score(y_test, tfidf_pred):.4f}\")\n",
    "    print(f\"F1 Score:  {f1_score(y_test, tfidf_pred):.4f}\")\n",
    "    print(f\"CV Mean:   {cv_tfidf_scores.mean():.4f} ± {cv_tfidf_scores.std():.4f}\")\n",
    "\n",
    "    print(\"\\nConfusion Matrix - BOW:\")\n",
    "    print(confusion_matrix(y_test, bow_pred))\n",
    "    print(\"\\nConfusion Matrix - TF-IDF:\")\n",
    "    print(confusion_matrix(y_test, tfidf_pred))\n",
    "\n",
    "    return bow_pred, tfidf_pred\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sklearn_implementation()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Exploring hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:\n",
      "  learning_rate: constant\n",
      "  eta0: 0.00010\n",
      "  alpha: 0.30353\n",
      "Best CV score: 86.2%\n",
      "Test accuracy: 84.0%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "import numpy as np\n",
    "\n",
    "X_raw, y = parser()\n",
    "y = (y + 1) // 2\n",
    "X = CountVectorizer(binary=True, max_features=10000).fit_transform(X_raw)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = RandomizedSearchCV(\n",
    "    SGDClassifier(loss='hinge', random_state=42),\n",
    "    {'learning_rate': ['constant', 'optimal', 'invscaling', 'adaptive'],\n",
    "     'eta0': np.logspace(-4, np.log10(3), 10),\n",
    "     'alpha': np.logspace(-4, np.log10(3), 10)},\n",
    "    n_iter=10, random_state=42\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters:\")\n",
    "for k, v in model.best_params_.items():\n",
    "    print(f\"  {k}: {v:.5f}\" if isinstance(v, float) else f\"  {k}: {v}\")\n",
    "print(f\"Best CV score: {model.best_score_ * 100:.1f}%\")\n",
    "print(f\"Test accuracy: {model.score(X_test, y_test) * 100:.1f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VG Part\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1.SGD \n",
    "1. Implement the optimization as *stochastic* gradient descent (SGD)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 2000\n",
      "Positive samples: 1000\n",
      "Negative samples: 1000\n",
      "   accuracy  precision    recall  f1_score\n",
      "0    0.8275   0.838542  0.809045  0.823529\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def parser(dataset_path='./txt_sentoken'):\n",
    "    X_raw, y = [], []\n",
    "    \n",
    "    pos_folder_path = os.path.join(dataset_path, 'pos')\n",
    "    for filename in os.listdir(pos_folder_path):\n",
    "        file_path = os.path.join(pos_folder_path, filename)\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\", errors='ignore') as file:\n",
    "            X_raw.append(file.read().strip().lower())\n",
    "        y.append(1)\n",
    "    \n",
    "    neg_folder_path = os.path.join(dataset_path, 'neg')\n",
    "    for filename in os.listdir(neg_folder_path):\n",
    "        file_path = os.path.join(neg_folder_path, filename)\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\", errors='ignore') as file:\n",
    "            X_raw.append(file.read().strip().lower())\n",
    "        y.append(-1)\n",
    "    \n",
    "    y = np.array(y)\n",
    "    \n",
    "    print(f\"Total samples: {len(X_raw)}\")\n",
    "    print(f\"Positive samples: {sum(y == 1)}\")\n",
    "    print(f\"Negative samples: {sum(y == -1)}\")\n",
    "    \n",
    "    return X_raw, y\n",
    "\n",
    "def train_and_evaluate(X_train, X_test, y_train, y_test):\n",
    "    pipeline = make_pipeline(\n",
    "        CountVectorizer(binary=True, max_features=10000),\n",
    "        SGDClassifier(loss='hinge', random_state=42)\n",
    "    )\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    classification_report = {\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'precision': precision_score(y_test, y_pred, zero_division=0),\n",
    "        'recall': recall_score(y_test, y_pred, zero_division=0),\n",
    "        'f1_score': f1_score(y_test, y_pred, zero_division=0)}\n",
    "    \n",
    "    return classification_report\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    X_raw, y = parser()\n",
    "    y_binary = (y + 1) // 2\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_raw, y_binary, test_size=0.2, random_state=42)\n",
    "    report = train_and_evaluate(X_train, X_test, y_train, y_test)\n",
    "    report_df = pd.DataFrame([report])\n",
    "    print(report_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    \n",
    "## 2.Tf-idf\n",
    "Implement a [tf-idf](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) feature model, and compare classification performance to bag-of-words (this should also be briefly discussed in your analysis). Choose your preferred formulation of tf-idf from the literature, *briefly* motivating your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 2000\n",
      "Positive samples: 1000\n",
      "Negative samples: 1000\n",
      "   accuracy  precision    recall  f1_score\n",
      "0    0.8525   0.864583  0.834171  0.849105\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def parser(dataset_path='./txt_sentoken'):\n",
    "    X_raw, y = [], []\n",
    "    \n",
    "    pos_folder_path = os.path.join(dataset_path, 'pos')\n",
    "    for filename in os.listdir(pos_folder_path):\n",
    "        file_path = os.path.join(pos_folder_path, filename)\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\", errors='ignore') as file:\n",
    "            X_raw.append(file.read().strip().lower())\n",
    "        y.append(1)\n",
    "    \n",
    "    neg_folder_path = os.path.join(dataset_path, 'neg')\n",
    "    for filename in os.listdir(neg_folder_path):\n",
    "        file_path = os.path.join(neg_folder_path, filename)\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\", errors='ignore') as file:\n",
    "            X_raw.append(file.read().strip().lower())\n",
    "        y.append(-1)\n",
    "    \n",
    "    y = np.array(y)\n",
    "    \n",
    "    print(f\"Total samples: {len(X_raw)}\")\n",
    "    print(f\"Positive samples: {sum(y == 1)}\")\n",
    "    print(f\"Negative samples: {sum(y == -1)}\")\n",
    "    \n",
    "    return X_raw, y\n",
    "\n",
    "def train_and_evaluate(X_train, X_test, y_train, y_test):\n",
    "    pipeline = make_pipeline(\n",
    "        TfidfVectorizer(binary=True, max_features=10000),\n",
    "        SGDClassifier(loss='hinge', random_state=42)\n",
    "    )\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    classification_report = {\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'precision': precision_score(y_test, y_pred, zero_division=0),\n",
    "        'recall': recall_score(y_test, y_pred, zero_division=0),\n",
    "        'f1_score': f1_score(y_test, y_pred, zero_division=0)}\n",
    "    \n",
    "    return classification_report\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    X_raw, y = parser()\n",
    "    y_binary = (y + 1) // 2\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_raw, y_binary, test_size=0.2, random_state=42)\n",
    "    report = train_and_evaluate(X_train, X_test, y_train, y_test)\n",
    "    report_df = pd.DataFrame([report])\n",
    "    print(report_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.adam     \n",
    "3. Implement an extension to the SDG optimization of your choice, e.g. from [this list on wikipedia](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Extensions_and_variants)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 2000\n",
      "Positive: 1000, Negative: 1000\n",
      "SGD + Optimal: 0.8520 ± 0.0076\n",
      "SGD + Adaptive: 0.8585 ± 0.0073\n",
      "SGD + Constant: 0.8605 ± 0.0066\n",
      "SGD + Invscaling: 0.5420 ± 0.0418\n",
      "LinearSVC: 0.8600 ± 0.0035\n",
      "SVM (RBF Kernel): 0.8415 ± 0.0044\n",
      "SVM (Poly Kernel): 0.8365 ± 0.0093\n",
      "Logistic Regression: 0.8250 ± 0.0072\n",
      "Multinomial NB: 0.8220 ± 0.0099\n",
      "Random Forest: 0.8060 ± 0.0161\n",
      "Decision Tree: 0.6425 ± 0.0195\n",
      "KNN: 0.6245 ± 0.0283\n",
      "\n",
      "Final Comparison:\n",
      "                  Model  CV Mean  CV Std\n",
      "2        SGD + Constant   0.8605  0.0066\n",
      "4             LinearSVC   0.8600  0.0035\n",
      "1        SGD + Adaptive   0.8585  0.0073\n",
      "0         SGD + Optimal   0.8520  0.0076\n",
      "5      SVM (RBF Kernel)   0.8415  0.0044\n",
      "6     SVM (Poly Kernel)   0.8365  0.0093\n",
      "7   Logistic Regression   0.8250  0.0072\n",
      "8        Multinomial NB   0.8220  0.0099\n",
      "9         Random Forest   0.8060  0.0161\n",
      "10        Decision Tree   0.6425  0.0195\n",
      "11                  KNN   0.6245  0.0283\n",
      "3      SGD + Invscaling   0.5420  0.0418\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def parser(dataset_path='./txt_sentoken'):\n",
    "    X_raw, y = [], []\n",
    "    pos_folder_path = os.path.join(dataset_path, 'pos')\n",
    "    for filename in os.listdir(pos_folder_path):\n",
    "        file_path = os.path.join(pos_folder_path, filename)\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\", errors='ignore') as file:\n",
    "            X_raw.append(file.read().strip().lower())\n",
    "        y.append(1)\n",
    "    neg_folder_path = os.path.join(dataset_path, 'neg')\n",
    "    for filename in os.listdir(neg_folder_path):\n",
    "        file_path = os.path.join(neg_folder_path, filename)\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\", errors='ignore') as file:\n",
    "            X_raw.append(file.read().strip().lower())\n",
    "        y.append(0)\n",
    "    y = np.array(y)\n",
    "    print(f\"Total samples: {len(X_raw)}\")\n",
    "    print(f\"Positive: {sum(y == 1)}, Negative: {sum(y == 0)}\")\n",
    "    return X_raw, y\n",
    "\n",
    "def compare_text_classifiers():\n",
    "    X_raw, y = parser()\n",
    "    models = {\n",
    "        'SGD + Optimal': make_pipeline(TfidfVectorizer(max_features=5000),\n",
    "                                       SGDClassifier(loss='hinge', learning_rate='optimal', max_iter=1000, random_state=42)),\n",
    "\n",
    "        'SGD + Adaptive': make_pipeline(TfidfVectorizer(max_features=5000),\n",
    "                                        SGDClassifier(loss='hinge', learning_rate='adaptive', eta0=0.01, max_iter=1000, random_state=42)),\n",
    "\n",
    "        'SGD + Constant': make_pipeline(TfidfVectorizer(max_features=5000),\n",
    "                                        SGDClassifier(loss='hinge', learning_rate='constant', eta0=0.01, max_iter=1000, random_state=42)),\n",
    "\n",
    "        'SGD + Invscaling': make_pipeline(TfidfVectorizer(max_features=5000),\n",
    "                                          SGDClassifier(loss='hinge', learning_rate='invscaling', eta0=0.01, max_iter=1000, random_state=42)),\n",
    "\n",
    "        'LinearSVC': make_pipeline(TfidfVectorizer(max_features=5000),\n",
    "                                   LinearSVC(C=1.0, random_state=42)),\n",
    "\n",
    "        'SVM (RBF Kernel)': make_pipeline(TfidfVectorizer(max_features=5000),\n",
    "                                          SVC(kernel='rbf', C=1.0, random_state=42)),\n",
    "\n",
    "        'SVM (Poly Kernel)': make_pipeline(TfidfVectorizer(max_features=5000),\n",
    "                                           SVC(kernel='poly', degree=3, C=1.0, random_state=42)),\n",
    "\n",
    "        'Logistic Regression': make_pipeline(TfidfVectorizer(max_features=5000),\n",
    "                                             LogisticRegression(max_iter=1000, random_state=42)),\n",
    "\n",
    "        'Multinomial NB': make_pipeline(TfidfVectorizer(max_features=5000),\n",
    "                                        MultinomialNB()),\n",
    "\n",
    "        'Random Forest': make_pipeline(TfidfVectorizer(max_features=5000),\n",
    "                                       RandomForestClassifier(n_estimators=100, random_state=42)),\n",
    "\n",
    "        'Decision Tree': make_pipeline(TfidfVectorizer(max_features=5000),\n",
    "                                       DecisionTreeClassifier(random_state=42)),\n",
    "\n",
    "        'KNN': make_pipeline(TfidfVectorizer(max_features=5000),\n",
    "                             KNeighborsClassifier(n_neighbors=5))\n",
    "    }\n",
    "\n",
    "    results = []\n",
    "    for name, model in models.items():\n",
    "        scores = cross_val_score(model, X_raw, y, cv=5, scoring='accuracy')\n",
    "        results.append({\n",
    "            'Model': name,\n",
    "            'CV Mean': scores.mean(),\n",
    "            'CV Std': scores.std()\n",
    "        })\n",
    "        print(f\"{name}: {scores.mean():.4f} ± {scores.std():.4f}\")\n",
    "\n",
    "    df = pd.DataFrame(results).sort_values(by='CV Mean', ascending=False)\n",
    "    print(\"\\nFinal Comparison:\")\n",
    "    print(df.round(4))\n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df_results = compare_text_classifiers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 4.k-fold cross validation\n",
    "1. Implement [k-fold cross validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics)#k-fold_cross-validation) for evaluating and comparing your model variants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 2000\n",
      "Positive: 1000, Negative: 1000\n",
      "===== Model Comparison =====\n",
      "BOW+SGD-Constant: 0.8430 ± 0.0099\n",
      "BOW+SGD-Optimal: 0.8360 ± 0.0129\n",
      "BOW+SGD-Adaptive: 0.8440 ± 0.0142\n",
      "BOW+SGD-Invscaling: 0.8550 ± 0.0164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/x/anaconda3/envs/nlp_env/lib/python3.9/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/x/anaconda3/envs/nlp_env/lib/python3.9/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOW+LinearSVC: 0.8405 ± 0.0091\n",
      "BOW+SVM-Linear: 0.8360 ± 0.0066\n",
      "BOW+SVM-RBF: 0.8540 ± 0.0216\n",
      "BOW+SVM-Poly: 0.8085 ± 0.0385\n",
      "BOW+LogisticRegression: 0.8575 ± 0.0076\n",
      "BOW+Multinomial NB: 0.8415 ± 0.0097\n",
      "BOW+RandomForest: 0.8150 ± 0.0148\n",
      "BOW+DecisionTree: 0.6150 ± 0.0173\n",
      "BOW+KNN: 0.5535 ± 0.0212\n",
      "TF-IDF+SGD-Constant: 0.8595 ± 0.0064\n",
      "TF-IDF+SGD-Optimal: 0.8470 ± 0.0075\n",
      "TF-IDF+SGD-Adaptive: 0.8595 ± 0.0060\n",
      "TF-IDF+SGD-Invscaling: 0.6390 ± 0.0861\n",
      "TF-IDF+LinearSVC: 0.8600 ± 0.0035\n",
      "TF-IDF+SVM-Linear: 0.8480 ± 0.0046\n",
      "TF-IDF+SVM-RBF: 0.8415 ± 0.0044\n",
      "TF-IDF+SVM-Poly: 0.8365 ± 0.0093\n",
      "TF-IDF+LogisticRegression: 0.8250 ± 0.0072\n",
      "TF-IDF+Multinomial NB: 0.8220 ± 0.0099\n",
      "TF-IDF+RandomForest: 0.8060 ± 0.0161\n",
      "TF-IDF+DecisionTree: 0.6425 ± 0.0195\n",
      "TF-IDF+KNN: 0.6245 ± 0.0283\n",
      "\n",
      "===== Complete Results =====\n",
      "                        Model  CV_Mean  CV_Std\n",
      "0            BOW+SGD-Constant   0.8430  0.0099\n",
      "1             BOW+SGD-Optimal   0.8360  0.0129\n",
      "2            BOW+SGD-Adaptive   0.8440  0.0142\n",
      "3          BOW+SGD-Invscaling   0.8550  0.0164\n",
      "4               BOW+LinearSVC   0.8405  0.0091\n",
      "5              BOW+SVM-Linear   0.8360  0.0066\n",
      "6                 BOW+SVM-RBF   0.8540  0.0216\n",
      "7                BOW+SVM-Poly   0.8085  0.0385\n",
      "8      BOW+LogisticRegression   0.8575  0.0076\n",
      "9          BOW+Multinomial NB   0.8415  0.0097\n",
      "10           BOW+RandomForest   0.8150  0.0148\n",
      "11           BOW+DecisionTree   0.6150  0.0173\n",
      "12                    BOW+KNN   0.5535  0.0212\n",
      "13        TF-IDF+SGD-Constant   0.8595  0.0064\n",
      "14         TF-IDF+SGD-Optimal   0.8470  0.0075\n",
      "15        TF-IDF+SGD-Adaptive   0.8595  0.0060\n",
      "16      TF-IDF+SGD-Invscaling   0.6390  0.0861\n",
      "17           TF-IDF+LinearSVC   0.8600  0.0035\n",
      "18          TF-IDF+SVM-Linear   0.8480  0.0046\n",
      "19             TF-IDF+SVM-RBF   0.8415  0.0044\n",
      "20            TF-IDF+SVM-Poly   0.8365  0.0093\n",
      "21  TF-IDF+LogisticRegression   0.8250  0.0072\n",
      "22      TF-IDF+Multinomial NB   0.8220  0.0099\n",
      "23        TF-IDF+RandomForest   0.8060  0.0161\n",
      "24        TF-IDF+DecisionTree   0.6425  0.0195\n",
      "25                 TF-IDF+KNN   0.6245  0.0283\n",
      "\n",
      "Best: TF-IDF+LinearSVC\n",
      "Accuracy: 0.8600 ± 0.0035\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def parser(dataset_path='./txt_sentoken'):\n",
    "    X_raw, y = [], []\n",
    "    pos_folder_path = os.path.join(dataset_path, 'pos')\n",
    "    for filename in os.listdir(pos_folder_path):\n",
    "        file_path = os.path.join(pos_folder_path, filename)\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\", errors='ignore') as file:\n",
    "            X_raw.append(file.read().strip().lower())\n",
    "        y.append(1)\n",
    "    neg_folder_path = os.path.join(dataset_path, 'neg')\n",
    "    for filename in os.listdir(neg_folder_path):\n",
    "        file_path = os.path.join(neg_folder_path, filename)\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\", errors='ignore') as file:\n",
    "            X_raw.append(file.read().strip().lower())\n",
    "        y.append(-1)\n",
    "    y = np.array(y)\n",
    "    print(f\"Total samples: {len(X_raw)}\")\n",
    "    print(f\"Positive: {sum(y == 1)}, Negative: {sum(y == -1)}\")\n",
    "    return X_raw, y\n",
    "\n",
    "def compare_all_methods():\n",
    "    X_raw, y = parser()\n",
    "    \n",
    "\n",
    "    classifiers = {\n",
    "        # BOW\n",
    "        'BOW+SGD-Constant': make_pipeline(CountVectorizer(binary=True, max_features=5000), SGDClassifier(loss='hinge', learning_rate='constant', eta0=0.01, max_iter=1000)),\n",
    "        'BOW+SGD-Optimal': make_pipeline(CountVectorizer(binary=True, max_features=5000), SGDClassifier(loss='hinge', learning_rate='optimal', max_iter=1000)),\n",
    "        'BOW+SGD-Adaptive': make_pipeline(CountVectorizer(binary=True, max_features=5000), SGDClassifier(loss='hinge', learning_rate='adaptive', eta0=0.01, max_iter=1000)),\n",
    "        'BOW+SGD-Invscaling': make_pipeline(CountVectorizer(binary=True, max_features=5000), SGDClassifier(loss='hinge', learning_rate='invscaling', eta0=0.01, max_iter=1000)),\n",
    "        'BOW+LinearSVC': make_pipeline(CountVectorizer(binary=True, max_features=5000), LinearSVC(C=1.0, random_state=42)),\n",
    "        'BOW+SVM-Linear': make_pipeline(CountVectorizer(binary=True, max_features=5000), SVC(kernel='linear', C=1.0, random_state=42)),\n",
    "        'BOW+SVM-RBF': make_pipeline(CountVectorizer(binary=True, max_features=5000), SVC(kernel='rbf', C=1.0, random_state=42)),\n",
    "        'BOW+SVM-Poly': make_pipeline(CountVectorizer(binary=True, max_features=5000), SVC(kernel='poly', degree=3, C=1.0, random_state=42)),\n",
    "        'BOW+LogisticRegression': make_pipeline(CountVectorizer(binary=True, max_features=5000), LogisticRegression(max_iter=1000, random_state=42)),\n",
    "        'BOW+Multinomial NB': make_pipeline(CountVectorizer(binary=True, max_features=5000), MultinomialNB()),\n",
    "        'BOW+RandomForest': make_pipeline(CountVectorizer(binary=True, max_features=5000), RandomForestClassifier(n_estimators=100, random_state=42)),\n",
    "        'BOW+DecisionTree': make_pipeline(CountVectorizer(binary=True, max_features=5000), DecisionTreeClassifier(random_state=42)),\n",
    "        'BOW+KNN': make_pipeline(CountVectorizer(binary=True, max_features=5000), KNeighborsClassifier(n_neighbors=5)),\n",
    "        \n",
    "        # TF-IDF\n",
    "        'TF-IDF+SGD-Constant': make_pipeline(TfidfVectorizer(max_features=5000), SGDClassifier(loss='hinge', learning_rate='constant', eta0=0.01, max_iter=1000)),\n",
    "        'TF-IDF+SGD-Optimal': make_pipeline(TfidfVectorizer(max_features=5000), SGDClassifier(loss='hinge', learning_rate='optimal', max_iter=1000)),\n",
    "        'TF-IDF+SGD-Adaptive': make_pipeline(TfidfVectorizer(max_features=5000), SGDClassifier(loss='hinge', learning_rate='adaptive', eta0=0.01, max_iter=1000)),\n",
    "        'TF-IDF+SGD-Invscaling': make_pipeline(TfidfVectorizer(max_features=5000), SGDClassifier(loss='hinge', learning_rate='invscaling', eta0=0.01, max_iter=1000)),\n",
    "        'TF-IDF+LinearSVC': make_pipeline(TfidfVectorizer(max_features=5000), LinearSVC(C=1.0, random_state=42)),\n",
    "        'TF-IDF+SVM-Linear': make_pipeline(TfidfVectorizer(max_features=5000), SVC(kernel='linear', C=1.0, random_state=42)),\n",
    "        'TF-IDF+SVM-RBF': make_pipeline(TfidfVectorizer(max_features=5000), SVC(kernel='rbf', C=1.0, random_state=42)),\n",
    "        'TF-IDF+SVM-Poly': make_pipeline(TfidfVectorizer(max_features=5000), SVC(kernel='poly', degree=3, C=1.0, random_state=42)),\n",
    "        'TF-IDF+LogisticRegression': make_pipeline(TfidfVectorizer(max_features=5000), LogisticRegression(max_iter=1000, random_state=42)),\n",
    "        'TF-IDF+Multinomial NB': make_pipeline(TfidfVectorizer(max_features=5000), MultinomialNB()),\n",
    "        'TF-IDF+RandomForest': make_pipeline(TfidfVectorizer(max_features=5000), RandomForestClassifier(n_estimators=100, random_state=42)),\n",
    "        'TF-IDF+DecisionTree': make_pipeline(TfidfVectorizer(max_features=5000), DecisionTreeClassifier(random_state=42)),\n",
    "        'TF-IDF+KNN': make_pipeline(TfidfVectorizer(max_features=5000), KNeighborsClassifier(n_neighbors=5))\n",
    "    }\n",
    "\n",
    "    results = []\n",
    "    print(\"===== Model Comparison =====\")\n",
    "    \n",
    "\n",
    "    for clf_name, classifier in classifiers.items():\n",
    "        cv_scores = cross_val_score(classifier, X_raw, y, cv=5, scoring='accuracy')\n",
    "        results.append({\n",
    "            'Model': clf_name,\n",
    "            'CV_Mean': cv_scores.mean(),\n",
    "            'CV_Std': cv_scores.std()\n",
    "        })\n",
    "        print(f\"{clf_name}: {cv_scores.mean():.4f} ± {cv_scores.std():.4f}\")\n",
    "    \n",
    "    df = pd.DataFrame(results)\n",
    "    print(\"\\n===== Complete Results =====\")\n",
    "    print(df.round(4))\n",
    "\n",
    "    best = df.loc[df['CV_Mean'].idxmax()]\n",
    "    print(f\"\\nBest: {best['Model']}\")\n",
    "    print(f\"Accuracy: {best['CV_Mean']:.4f} ± {best['CV_Std']:.4f}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    results = compare_all_methods()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 2000\n",
      "\n",
      "===== TF-IDF+SGD =====\n",
      "Best Score: 0.8635\n",
      "Best Parameters: {'sgdclassifier__alpha': 0.0001, 'sgdclassifier__eta0': 0.01, 'tfidfvectorizer__max_features': 10000, 'tfidfvectorizer__ngram_range': (1, 2)}\n",
      "\n",
      "===== TF-IDF+LR =====\n",
      "Best Score: 0.8690\n",
      "Best Parameters: {'logisticregression__C': 10.0, 'tfidfvectorizer__max_features': 10000, 'tfidfvectorizer__ngram_range': (1, 2)}\n",
      "\n",
      "===== BOW+SVM =====\n",
      "Best Score: 0.8565\n",
      "Best Parameters: {'countvectorizer__max_features': 5000, 'svc__C': 10.0, 'svc__kernel': 'rbf'}\n",
      "\n",
      "===== final ranking =====\n",
      "TF-IDF+LR: 0.8690\n",
      "TF-IDF+SGD: 0.8635\n",
      "BOW+SVM: 0.8565\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def parser(dataset_path='./txt_sentoken'):\n",
    "    X_raw, y = [], []\n",
    "    pos_folder_path = os.path.join(dataset_path, 'pos')\n",
    "    for filename in os.listdir(pos_folder_path):\n",
    "        file_path = os.path.join(pos_folder_path, filename)\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\", errors='ignore') as file:\n",
    "            X_raw.append(file.read().strip().lower())\n",
    "        y.append(1)\n",
    "    neg_folder_path = os.path.join(dataset_path, 'neg')\n",
    "    for filename in os.listdir(neg_folder_path):\n",
    "        file_path = os.path.join(neg_folder_path, filename)\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\", errors='ignore') as file:\n",
    "            X_raw.append(file.read().strip().lower())\n",
    "        y.append(-1)\n",
    "    return X_raw, np.array(y)\n",
    "\n",
    "def simple_grid_search():\n",
    "    X_raw, y = parser()\n",
    "    print(f\"Total samples: {len(X_raw)}\")\n",
    "    \n",
    "\n",
    "    models = {\n",
    "        'TF-IDF+SGD': {\n",
    "            'pipeline': make_pipeline(TfidfVectorizer(), SGDClassifier(max_iter=2000)),\n",
    "            'params': {\n",
    "                'tfidfvectorizer__max_features': [5000, 10000],\n",
    "                'tfidfvectorizer__ngram_range': [(1,1), (1,2)],\n",
    "                'sgdclassifier__alpha': [0.0001, 0.001],\n",
    "                'sgdclassifier__eta0': [0.01]\n",
    "            }\n",
    "        },\n",
    "        'TF-IDF+LR': {\n",
    "            'pipeline': make_pipeline(TfidfVectorizer(), LogisticRegression(max_iter=2000)),\n",
    "            'params': {\n",
    "                'tfidfvectorizer__max_features': [5000, 10000],\n",
    "                'tfidfvectorizer__ngram_range': [(1,1), (1,2)],\n",
    "                'logisticregression__C': [1.0, 10.0]\n",
    "            }\n",
    "        },\n",
    "        'BOW+SVM': {\n",
    "            'pipeline': make_pipeline(CountVectorizer(binary=True), SVC()),\n",
    "            'params': {\n",
    "                'countvectorizer__max_features': [5000],\n",
    "                'svc__kernel': ['linear', 'rbf'],\n",
    "                'svc__C': [1.0, 10.0]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    results = []\n",
    "    for name, config in models.items():\n",
    "        print(f\"\\n===== {name} =====\")\n",
    "        grid = GridSearchCV(config['pipeline'], config['params'], cv=5, scoring='accuracy')\n",
    "        grid.fit(X_raw, y)\n",
    "        results.append((name, grid.best_score_, grid.best_params_))\n",
    "        print(f\"Best Score: {grid.best_score_:.4f}\")\n",
    "        print(f\"Best Parameters: {grid.best_params_}\")\n",
    "    \n",
    "\n",
    "    print(\"\\n===== final ranking =====\")\n",
    "    for name, score, params in sorted(results, key=lambda x: x[1], reverse=True):\n",
    "        print(f\"{name}: {score:.4f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    results = simple_grid_search()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
