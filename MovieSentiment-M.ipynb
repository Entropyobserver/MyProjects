{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **MovieSentiment-ML**\n",
    "*A From-Scratch Movie Review Sentiment Analysis System*\n",
    "\n",
    "---\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "This is a **completely hand-implemented** machine learning project that builds a sentiment analysis system from the ground up, without relying on any existing ML libraries (like sklearn). Using the Cornell University Review Polarity v2.0 dataset containing 2000 movie reviews, it implements a complete text classification pipeline with pure Python and mathematical foundations.\n",
    "\n",
    "### Key Highlights\n",
    "\n",
    " **Pure Hand-Implementation**\n",
    "- Zero dependency on third-party ML libraries, demonstrating deep understanding of machine learning algorithms\n",
    "- Every component from linear algebra operations to optimization algorithms is self-coded\n",
    "\n",
    " **Complete ML Pipeline**\n",
    "```\n",
    "Raw Text → Feature Extraction → Model Training → Evaluation & Prediction\n",
    "```\n",
    "\n",
    "**Comprehensive Technical Comparison**\n",
    "- **Feature Engineering**: Binary BOW vs TF-IDF\n",
    "- **Optimization Algorithms**: Batch Gradient Descent vs SGD vs Adam\n",
    "- **Model Evaluation**: Single Training vs K-Fold Cross Validation\n",
    "\n",
    "## Technical Implementation\n",
    "\n",
    "**1. Data Processing Module**\n",
    "```python\n",
    "def parser(dataset_path): \n",
    "    # Automatically parses positive/negative samples, returns (X_raw, y) format\n",
    "```\n",
    "\n",
    "**2. Feature Extraction Engine**\n",
    "- **CountVectorizer**: Binary bag-of-words model focusing on word presence/absence\n",
    "- **TfidfVectorizer**: TF-IDF weighting to emphasize important words\n",
    "- Optimized memory usage with sparse matrix implementation\n",
    "\n",
    "**3. Machine Learning Core**\n",
    "- **SVMClassifier**: Support Vector Machine using Hinge Loss\n",
    "- **SGDClassifier**: Stochastic Gradient Descent optimization\n",
    "- **Adam Optimizer**: Adaptive learning rate algorithm\n",
    "\n",
    "**4. Model Evaluation System**\n",
    "- Hand-written K-fold cross validation\n",
    "- Multi-metric evaluation (Accuracy, Precision, Recall, F1)\n",
    "- Hyperparameter grid search\n",
    "\n",
    "## Experimental Results\n",
    "\n",
    "Through comparative experiments, we found:\n",
    "- **TF-IDF + SGD**: Best performance with 85%+ accuracy\n",
    "- **Adam Optimizer**: Faster and more stable convergence than vanilla SGD\n",
    "- **K-Fold Validation**: Provides more reliable performance assessment\n",
    "\n",
    "## Technical Value\n",
    "\n",
    "**Deep Algorithmic Understanding**\n",
    "Demonstrates mastery of core ML concepts:\n",
    "- Loss function design and optimization\n",
    "- Gradient computation and backpropagation\n",
    "- Regularization and overfitting control\n",
    "\n",
    "**Engineering Excellence**\n",
    "- Efficient sparse matrix processing\n",
    "- Modular code architecture\n",
    "- Comprehensive experimental comparison framework\n",
    "\n",
    "**Research Methodology**\n",
    "- Systematic ablation experiments\n",
    "- Multi-perspective performance analysis\n",
    "- Scientific experimental design\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Parsing the dataset\n",
    "\n",
    "**Implementation task:** Implement a parser for the dataset. The output should be a list/array of strings (`X_raw`) and a list/array of labels (`y`) encoded as {-1,1}.\n",
    "\n",
    "Dataset Structure: Review Polarity v2.0\n",
    "\n",
    "Positive Reviews: 1000\n",
    "\n",
    "Negative Reviews: 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 2000\n",
      "Positive samples: 1000\n",
      "Negative samples: 1000\n",
      "films adapted from comic books have had plenty of success , whether they're about superheroes ( batman , superman , spawn ) , or geared toward kids ( casper ) or the arthouse crowd ( ghost world ) , but there's never really been a comic book like from hell before . \n",
      "for starters , it was created by alan moore ( and eddie campbell ) , who brought the medium to a whole new level in the mid '80s with a 12-part series called the watchmen . \n",
      "to say moore and campbell thoroughly researched the subject of jack the ripper would be like saying michael jackson is starting to look a little odd . \n",
      "the book ( or \" graphic novel , \" if you will ) is over 500 pages long and includes nearly 30 more that consist of nothing but footnotes . \n",
      "in other words , don't dismiss this film because of its source . \n",
      "if you can get past the whole comic book thing , you might find another stumbling block in from hell's directors , albert and allen hughes . \n",
      "getting the hughes brothers to direct this seems almost as ludicrous as casting carrot top in , well , anything , but riddle me this : who better to direct a film that's set in the ghetto and features really violent street crime than the mad geniuses behind menace ii society ? \n",
      "the ghetto in question is , of course , whitechapel in 1888 london's east end . \n",
      "it's a filthy , sooty place where the whores ( called \" unfortunates \" ) are starting to get a little nervous about this mysterious psychopath who has been carving through their profession with surgical precision . \n",
      "when the first stiff turns up , copper peter godley ( robbie coltrane , the world is not enough ) calls in inspector frederick abberline ( johnny depp , blow ) to crack the case . \n",
      "abberline , a widower , has prophetic dreams he unsuccessfully tries to quell with copious amounts of absinthe and opium . \n",
      "upon arriving in whitechapel , he befriends an unfortunate named mary kelly ( heather graham , say it isn't so ) and proceeds to investigate the horribly gruesome crimes that even the police surgeon can't stomach . \n",
      "i don't think anyone needs to be briefed on jack the ripper , so i won't go into the particulars here , other than to say moore and campbell have a unique and interesting theory about both the identity of the killer and the reasons he chooses to slay . \n",
      "in the comic , they don't bother cloaking the identity of the ripper , but screenwriters terry hayes ( vertical limit ) and rafael yglesias ( les mis ? rables ) do a good job of keeping him hidden from viewers until the very end . \n",
      "it's funny to watch the locals blindly point the finger of blame at jews and indians because , after all , an englishman could never be capable of committing such ghastly acts . \n",
      "and from hell's ending had me whistling the stonecutters song from the simpsons for days ( \" who holds back the electric car/who made steve guttenberg a star ? \" ) . \n",
      "don't worry - it'll all make sense when you see it . \n",
      "now onto from hell's appearance : it's certainly dark and bleak enough , and it's surprising to see how much more it looks like a tim burton film than planet of the apes did ( at times , it seems like sleepy hollow 2 ) . \n",
      "the print i saw wasn't completely finished ( both color and music had not been finalized , so no comments about marilyn manson ) , but cinematographer peter deming ( don't say a word ) ably captures the dreariness of victorian-era london and helped make the flashy killing scenes remind me of the crazy flashbacks in twin peaks , even though the violence in the film pales in comparison to that in the black-and-white comic . \n",
      "oscar winner martin childs' ( shakespeare in love ) production design turns the original prague surroundings into one creepy place . \n",
      "even the acting in from hell is solid , with the dreamy depp turning in a typically strong performance and deftly handling a british accent . \n",
      "ians holm ( joe gould's secret ) and richardson ( 102 dalmatians ) log in great supporting roles , but the big surprise here is graham . \n",
      "i cringed the first time she opened her mouth , imagining her attempt at an irish accent , but it actually wasn't half bad . \n",
      "the film , however , is all good . \n",
      "2 : 00 - r for strong violence/gore , sexuality , language and drug content\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def parser(dataset_path='./txt_sentoken'):\n",
    "    X_raw, y = [], []\n",
    "    \n",
    "    pos_folder_path = os.path.join(dataset_path, 'pos')\n",
    "    for filename in os.listdir(pos_folder_path):\n",
    "        file_path = os.path.join(pos_folder_path, filename)\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\", errors='ignore') as file:\n",
    "            X_raw.append(file.read().strip().lower())\n",
    "        y.append(1)\n",
    "    \n",
    "\n",
    "    neg_folder_path = os.path.join(dataset_path, 'neg')\n",
    "    for filename in os.listdir(neg_folder_path):\n",
    "        file_path = os.path.join(neg_folder_path, filename)\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\", errors='ignore') as file:\n",
    "            X_raw.append(file.read().strip().lower())\n",
    "        y.append(-1)\n",
    "    \n",
    "    y = np.array(y)\n",
    "\n",
    "    print(f\"Total samples: {len(X_raw)}\")\n",
    "    print(f\"Positive samples: {sum(y == 1)}\")\n",
    "    print(f\"Negative samples: {sum(y == -1)}\")\n",
    "\n",
    "    return X_raw, y\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    X_raw, y = parser()\n",
    "    print(X_raw[0])  \n",
    "    print(y[0])  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Feature extraction\n",
    "\n",
    "**Implementation task:** You should re-implement the feature extraction above. The list/array called `ordered_vocabulary` should contain the words for each feature dimension, and X should contain the BOW binary vectors. Remember to use the same method names as the original sklearn class.\n",
    "\n",
    "*Hints: Implementing X as a NumPy array or a SciPy sparse matrix (not as a list) will make your life easier in the coming parts. Also, the `in` operator is way faster for sets than for lists.*\n",
    "\n",
    "We can now look at the data and the words corresponding to feature dimensions.\n",
    "\n",
    "Why Use Binary Bag-of-Words (BOW)?\n",
    "\n",
    "Advantages:\n",
    "\n",
    "Reduces the impact of high-frequency words: For example, if \"movie\" appears 100 times, its influence won't overshadow less frequent but meaningful words like \"great.\"\n",
    "\n",
    "Well-suited for classification tasks: Focuses on the presence of words rather than their frequency.\n",
    "\n",
    "Minimizes the effect of common words: Words like \"the,\" \"is,\" and \"and\" won't disproportionately affect the model's performance.\n",
    "\n",
    "Disadvantages:\n",
    "\n",
    "Loses frequency information: This can reduce accuracy for tasks like topic modeling or information retrieval that rely on word frequency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 2000\n",
      "Positive samples: 1000\n",
      "Negative samples: 1000\n",
      "Sparse Matrix Shape (Custom): (2000, 50920)\n",
      "First sentence: films adapted from comic books have had plenty of success , whether they're about superheroes ( batman , superman , spawn ) , or geared toward kids ( casper ) or the arthouse crowd ( ghost world ) , but there's never really been a comic book like from hell before . \n",
      "for starters , it was created by alan moore ( and eddie campbell ) , who brought the medium to a whole new level in the mid '80s with a 12-part series called the watchmen . \n",
      "to say moore and campbell thoroughly researched the subject of jack the ripper would be like saying michael jackson is starting to look a little odd . \n",
      "the book ( or \" graphic novel , \" if you will ) is over 500 pages long and includes nearly 30 more that consist of nothing but footnotes . \n",
      "in other words , don't dismiss this film because of its source . \n",
      "if you can get past the whole comic book thing , you might find another stumbling block in from hell's directors , albert and allen hughes . \n",
      "getting the hughes brothers to direct this seems almost as ludicrous as casting carrot top in , well , anything , but riddle me this : who better to direct a film that's set in the ghetto and features really violent street crime than the mad geniuses behind menace ii society ? \n",
      "the ghetto in question is , of course , whitechapel in 1888 london's east end . \n",
      "it's a filthy , sooty place where the whores ( called \" unfortunates \" ) are starting to get a little nervous about this mysterious psychopath who has been carving through their profession with surgical precision . \n",
      "when the first stiff turns up , copper peter godley ( robbie coltrane , the world is not enough ) calls in inspector frederick abberline ( johnny depp , blow ) to crack the case . \n",
      "abberline , a widower , has prophetic dreams he unsuccessfully tries to quell with copious amounts of absinthe and opium . \n",
      "upon arriving in whitechapel , he befriends an unfortunate named mary kelly ( heather graham , say it isn't so ) and proceeds to investigate the horribly gruesome crimes that even the police surgeon can't stomach . \n",
      "i don't think anyone needs to be briefed on jack the ripper , so i won't go into the particulars here , other than to say moore and campbell have a unique and interesting theory about both the identity of the killer and the reasons he chooses to slay . \n",
      "in the comic , they don't bother cloaking the identity of the ripper , but screenwriters terry hayes ( vertical limit ) and rafael yglesias ( les mis ? rables ) do a good job of keeping him hidden from viewers until the very end . \n",
      "it's funny to watch the locals blindly point the finger of blame at jews and indians because , after all , an englishman could never be capable of committing such ghastly acts . \n",
      "and from hell's ending had me whistling the stonecutters song from the simpsons for days ( \" who holds back the electric car/who made steve guttenberg a star ? \" ) . \n",
      "don't worry - it'll all make sense when you see it . \n",
      "now onto from hell's appearance : it's certainly dark and bleak enough , and it's surprising to see how much more it looks like a tim burton film than planet of the apes did ( at times , it seems like sleepy hollow 2 ) . \n",
      "the print i saw wasn't completely finished ( both color and music had not been finalized , so no comments about marilyn manson ) , but cinematographer peter deming ( don't say a word ) ably captures the dreariness of victorian-era london and helped make the flashy killing scenes remind me of the crazy flashbacks in twin peaks , even though the violence in the film pales in comparison to that in the black-and-white comic . \n",
      "oscar winner martin childs' ( shakespeare in love ) production design turns the original prague surroundings into one creepy place . \n",
      "even the acting in from hell is solid , with the dreamy depp turning in a typically strong performance and deftly handling a british accent . \n",
      "ians holm ( joe gould's secret ) and richardson ( 102 dalmatians ) log in great supporting roles , but the big surprise here is graham . \n",
      "i cringed the first time she opened her mouth , imagining her attempt at an irish accent , but it actually wasn't half bad . \n",
      "the film , however , is all good . \n",
      "2 : 00 - r for strong violence/gore , sexuality , language and drug content\n",
      "First sentence vector (Custom, Sparse):   (0, 11)\t1\n",
      "  (0, 110)\t1\n",
      "  (0, 474)\t1\n",
      "  (0, 475)\t1\n",
      "  (0, 484)\t1\n",
      "  (0, 485)\t1\n",
      "  (0, 576)\t1\n",
      "  (0, 586)\t1\n",
      "  (0, 642)\t1\n",
      "  (0, 678)\t1\n",
      "  (0, 789)\t1\n",
      "  (0, 940)\t1\n",
      "  (0, 1068)\t1\n",
      "  (0, 1168)\t1\n",
      "  (0, 1333)\t1\n",
      "  (0, 1350)\t1\n",
      "  (0, 1916)\t1\n",
      "  (0, 1963)\t1\n",
      "  (0, 1997)\t1\n",
      "  (0, 2023)\t1\n",
      "  (0, 2054)\t1\n",
      "  (0, 2112)\t1\n",
      "  (0, 2260)\t1\n",
      "  (0, 2329)\t1\n",
      "  (0, 2335)\t1\n",
      "  :\t:\n",
      "  (0, 49181)\t1\n",
      "  (0, 49196)\t1\n",
      "  (0, 49211)\t1\n",
      "  (0, 49223)\t1\n",
      "  (0, 49442)\t1\n",
      "  (0, 49647)\t1\n",
      "  (0, 49651)\t1\n",
      "  (0, 49660)\t1\n",
      "  (0, 49711)\t1\n",
      "  (0, 49727)\t1\n",
      "  (0, 49747)\t1\n",
      "  (0, 49765)\t1\n",
      "  (0, 49786)\t1\n",
      "  (0, 49825)\t1\n",
      "  (0, 49874)\t1\n",
      "  (0, 49973)\t1\n",
      "  (0, 50064)\t1\n",
      "  (0, 50159)\t1\n",
      "  (0, 50224)\t1\n",
      "  (0, 50234)\t1\n",
      "  (0, 50271)\t1\n",
      "  (0, 50304)\t1\n",
      "  (0, 50327)\t1\n",
      "  (0, 50633)\t1\n",
      "  (0, 50681)\t1\n",
      "\n",
      "First sentence vectorized using dense matrix (Custom):\n",
      " [[0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse\n",
    "\n",
    "class CountVectorizer:\n",
    "    def __init__(self):\n",
    "        self.vocabulary_ = None\n",
    "        self.ordered_vocabulary = None\n",
    "    \n",
    "    def fit(self, X_raw):\n",
    "        words_set = set()\n",
    "        for text in X_raw:\n",
    "            words_set.update(text.lower().split())\n",
    "        self.ordered_vocabulary = sorted(words_set)\n",
    "        self.vocabulary_ = {}\n",
    "        for i, word in enumerate(self.ordered_vocabulary):\n",
    "            self.vocabulary_[word] = i\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X_raw):\n",
    "        rows = []\n",
    "        cols = []\n",
    "        data = []\n",
    "        for i, text in enumerate(X_raw):\n",
    "            # use set to avoid duplicates\n",
    "            for word in set(text.lower().split()):\n",
    "                if word in self.vocabulary_:\n",
    "                    rows.append(i)\n",
    "                    cols.append(self.vocabulary_[word])\n",
    "                    data.append(1)  # only mark 1 as presence\n",
    "        X = sparse.csr_matrix((data, (rows, cols)), shape=(len(X_raw), len(self.vocabulary_)))\n",
    "        return X\n",
    "    \n",
    "    def fit_transform(self, X_raw):\n",
    "        self.fit(X_raw)\n",
    "        return self.transform(X_raw)\n",
    "    \n",
    "    def get_feature_names_out(self):\n",
    "        return self.ordered_vocabulary\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    X_raw, y = parser() \n",
    "    vectorizer = CountVectorizer()\n",
    "    X = vectorizer.fit_transform(X_raw)\n",
    "    print(\"Sparse Matrix Shape (Custom):\", X.shape)\n",
    "    first_sentence = X_raw[0]\n",
    "    print(\"First sentence:\", first_sentence)\n",
    "    first_sentence_vector = X[0]\n",
    "    print(\"First sentence vector (Custom, Sparse):\", first_sentence_vector)\n",
    "    first_sentence_dense = first_sentence_vector.toarray()\n",
    "    print(\"\\nFirst sentence vectorized using dense matrix (Custom):\\n\", first_sentence_dense)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Learning framework\n",
    "\n",
    "The main goal is to implement these components (the model, the loss function, and gradient descent) and iteratively train the model until it converges.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 2000\n",
      "Positive samples: 1000\n",
      "Negative samples: 1000\n",
      "     Metrics  Values\n",
      "0   Accuracy  0.8550\n",
      "1  Precision  0.8505\n",
      "2     Recall  0.8750\n",
      "3   F1_Score  0.8626\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "import os\n",
    "\n",
    "\n",
    "def parser(dataset_path='./txt_sentoken'):\n",
    "    X_raw, y = [], []\n",
    "    \n",
    "    pos_folder_path = os.path.join(dataset_path, 'pos')\n",
    "    for filename in os.listdir(pos_folder_path):\n",
    "        file_path = os.path.join(pos_folder_path, filename)\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\", errors='ignore') as file:\n",
    "            X_raw.append(file.read().strip().lower())\n",
    "        y.append(1)\n",
    "    \n",
    "\n",
    "    neg_folder_path = os.path.join(dataset_path, 'neg')\n",
    "    for filename in os.listdir(neg_folder_path):\n",
    "        file_path = os.path.join(neg_folder_path, filename)\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\", errors='ignore') as file:\n",
    "            X_raw.append(file.read().strip().lower())\n",
    "        y.append(-1)\n",
    "    \n",
    "    y = np.array(y)\n",
    "\n",
    "    print(f\"Total samples: {len(X_raw)}\")\n",
    "    print(f\"Positive samples: {sum(y == 1)}\")\n",
    "    print(f\"Negative samples: {sum(y == -1)}\")\n",
    "\n",
    "    return X_raw, y\n",
    "\n",
    "class CountVectorizer:\n",
    "    def __init__(self):\n",
    "        self.vocabulary_ = None\n",
    "        self.ordered_vocabulary = None\n",
    "    \n",
    "    def fit(self, X_raw):\n",
    "        words_set = set()\n",
    "        for text in X_raw:\n",
    "            words_set.update(text.lower().split())\n",
    "        self.ordered_vocabulary = sorted(words_set)\n",
    "        self.vocabulary_ = {}\n",
    "        for i, word in enumerate(self.ordered_vocabulary):\n",
    "            self.vocabulary_[word] = i\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X_raw):\n",
    "        rows = []\n",
    "        cols = []\n",
    "        data = []\n",
    "        for i, text in enumerate(X_raw):\n",
    "            for word in set(text.lower().split()):\n",
    "                if word in self.vocabulary_:\n",
    "                    rows.append(i)\n",
    "                    cols.append(self.vocabulary_[word])\n",
    "                    data.append(1)\n",
    "        X = sparse.csr_matrix((data, (rows, cols)), shape=(len(X_raw), len(self.vocabulary_)))\n",
    "        return X\n",
    "    \n",
    "    def fit_transform(self, X_raw):\n",
    "        self.fit(X_raw)\n",
    "        return self.transform(X_raw)\n",
    "    \n",
    "    def get_feature_names_out(self):\n",
    "        return self.ordered_vocabulary\n",
    "\n",
    "class SVMClassifier:\n",
    "    def __init__(self, learning_rate=0.01, reguliser_dampening=0.01, max_iterations=1000, tolerance=1e-5):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.reguliser_dampening = reguliser_dampening\n",
    "        self.max_iterations = max_iterations\n",
    "        self.tolerance = tolerance\n",
    "        self.omega = None\n",
    "        self.loss_history = []\n",
    "        \n",
    "    def hinge_loss(self, X, y):\n",
    "        reg_term = (self.reguliser_dampening / 2) * np.sum(self.omega[1:]**2)\n",
    "        margins = y * (X.dot(self.omega))\n",
    "        losses = np.maximum(0, 1 - margins)\n",
    "        hinge_term = np.mean(losses)\n",
    "        return reg_term + hinge_term\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        X_arr = X.toarray() if sparse.issparse(X) else X\n",
    "        self.omega = np.zeros(n_features + 1)\n",
    "        X_with_bias = np.hstack((np.ones((n_samples, 1)), X_arr))\n",
    "        \n",
    "        for epoch in range(self.max_iterations):\n",
    "            current_loss = self.hinge_loss(X_with_bias, y)\n",
    "            self.loss_history.append(current_loss)\n",
    "            \n",
    "            gradient = np.zeros_like(self.omega)\n",
    "            gradient[1:] = self.reguliser_dampening * self.omega[1:]\n",
    "            \n",
    "            margins = y * np.dot(X_with_bias, self.omega)\n",
    "            mask = margins < 1\n",
    "            if np.any(mask):\n",
    "                hinge_gradient = -np.sum((y[mask].reshape(-1, 1) * X_with_bias[mask]), axis=0) / n_samples\n",
    "                gradient += hinge_gradient\n",
    "            \n",
    "            self.omega -= self.learning_rate * gradient\n",
    "            \n",
    "            #if epoch > 0 and abs(self.loss_history[-1] - self.loss_history[-2]) < self.tolerance:\n",
    "            #    break\n",
    "            if epoch > 0 and abs(self.loss_history[-2]) > 1e-10:\n",
    "                relative_change = abs((self.loss_history[-1] - self.loss_history[-2]) / self.loss_history[-2])\n",
    "                if relative_change < self.tolerance:\n",
    "                    break   \n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X_arr = X.toarray() if sparse.issparse(X) else X\n",
    "        n_samples = X_arr.shape[0]\n",
    "        X_with_bias = np.hstack((np.ones((n_samples, 1)), X_arr))\n",
    "        return np.sign(np.dot(X_with_bias, self.omega))\n",
    "\n",
    "    def score(self, X, y):\n",
    "        y_pred = self.predict(X)\n",
    "        tp = np.sum((y == 1) & (y_pred == 1))\n",
    "        fp = np.sum((y == -1) & (y_pred == 1))\n",
    "        tn = np.sum((y == -1) & (y_pred == -1))\n",
    "        fn = np.sum((y == 1) & (y_pred == -1))\n",
    "        accuracy = (tp + tn) / (tp + fp + tn + fn)\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        metrics = {\n",
    "            \"Metrics\": [\"Accuracy\", \"Precision\", \"Recall\", \"F1_Score\"],\n",
    "            \"Values\": [accuracy, precision, recall, f1]\n",
    "        }\n",
    "        df = pd.DataFrame(metrics)\n",
    "        df[\"Values\"] = df[\"Values\"].round(4)\n",
    "        return df\n",
    "\n",
    "\n",
    "def manual_train_test_split(X, y, train_size=0.8):\n",
    "    n_samples = X.shape[0] # Get the number of samples\n",
    "    train_samples = int(n_samples * train_size) # Calculate the number of training samples\n",
    "    indices = np.arange(n_samples) # Create an array of indices\n",
    "    np.random.shuffle(indices)# Shuffle the indices\n",
    "    train_indices = indices[:train_samples]# Select the training indices\n",
    "    test_indices = indices[train_samples:]# Select the test indices\n",
    "    return X[train_indices], X[test_indices], y[train_indices], y[test_indices]\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    X_raw, y = parser()\n",
    "    vectorizer = CountVectorizer()\n",
    "    vectorizer.fit(X_raw)\n",
    "    X = vectorizer.transform(X_raw)\n",
    "    X_train, X_test, y_train, y_test = manual_train_test_split(X, y)\n",
    "    gd_classifier = SVMClassifier(learning_rate=0.01, reguliser_dampening=0.001, max_iterations=1000)\n",
    "    gd_classifier.fit(X_train, y_train)\n",
    "    predictions = gd_classifier.predict(X_test)\n",
    "    results = gd_classifier.score(X_test, y_test)\n",
    "    print(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Exploring hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:\tReg.dampening:\tTraining set accuracy:\n",
      "0.00311\t\t0.00311\t\t96.2%\n",
      "0.00031\t\t0.03071\t\t87.9%\n",
      "0.00031\t\t0.00311\t\t87.9%\n",
      "0.00977\t\t0.95425\t\t89.1%\n",
      "0.03071\t\t0.30353\t\t95.6%\n",
      "0.03071\t\t0.00977\t\t100.0%\n",
      "0.00311\t\t0.00010\t\t96.2%\n",
      "0.00031\t\t0.30353\t\t87.9%\n",
      "0.00099\t\t0.00099\t\t88.9%\n",
      "0.09655\t\t0.00031\t\t100.0%\n",
      "Best parameters: 0.03071, 0.00977\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import ParameterSampler\n",
    "\n",
    "# Define hyperparameters\n",
    "parameter_distribution = {'learning_rate': np.exp(np.linspace(np.log(0.0001), np.log(3), 10)),\n",
    "                          'reguliser_dampening': np.exp(np.linspace(np.log(0.0001), np.log(3), 10))}\n",
    "\n",
    "# Placeholder for storing the best hyperparameters and training accuracy\n",
    "best_hyperparameters = None\n",
    "print(\"Learning rate:\\tReg.dampening:\\tTraining set accuracy:\")\n",
    "\n",
    "# Use ParameterSampler to randomly select hyperparameters\n",
    "for hyperparameters in ParameterSampler(parameter_distribution, n_iter=10):  # **Keep this line unchanged**\n",
    "    # Extract learning rate and regularization dampening from sampled parameters\n",
    "    learning_rate = hyperparameters['learning_rate']\n",
    "    reguliser_dampening = hyperparameters['reguliser_dampening']\n",
    "    \n",
    "    # Create a model instance (SVMClassifier in this case)\n",
    "    model = SVMClassifier(learning_rate=learning_rate, reguliser_dampening=reguliser_dampening)\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Calculate training accuracy\n",
    "    training_accuracy = np.sum(model.predict(X_train) == y_train) / len(y_train)\n",
    "\n",
    "    # Store the best hyperparameters\n",
    "    if best_hyperparameters is None or best_hyperparameters[1] < training_accuracy:\n",
    "        best_hyperparameters = (hyperparameters, training_accuracy)\n",
    "    \n",
    "    # Print current hyperparameters and training accuracy\n",
    "    print(\"%.5f\\t\\t%.5f\\t\\t%.1f%%\" % (hyperparameters['learning_rate'], \n",
    "                                      hyperparameters['reguliser_dampening'], \n",
    "                                      100 * training_accuracy))\n",
    "\n",
    "# Output the best hyperparameters\n",
    "best_learning_rate = best_hyperparameters[0]['learning_rate']\n",
    "best_reguliser_dampening = best_hyperparameters[0]['reguliser_dampening']\n",
    "print(\"Best parameters: %.5f, %.5f\" % (best_learning_rate, best_reguliser_dampening))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 2000\n",
      "Positive samples: 1000\n",
      "Negative samples: 1000\n",
      "     Metrics  Values\n",
      "0   Accuracy  0.8225\n",
      "1  Precision  0.8283\n",
      "2     Recall  0.8159\n",
      "3   F1_Score  0.8221\n"
     ]
    }
   ],
   "source": [
    "best_learning_rate = 0.03071\n",
    "best_reguliser_dampening = 0.00977\n",
    "\n",
    "X_raw, y = parser()\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(X_raw)\n",
    "X = vectorizer.transform(X_raw)\n",
    "X_train, X_test, y_train, y_test = manual_train_test_split(X, y)\n",
    "\n",
    "gd_classifier = SVMClassifier(learning_rate=best_learning_rate, \n",
    "                              reguliser_dampening=best_reguliser_dampening, \n",
    "                              max_iterations=1000)\n",
    "\n",
    "gd_classifier.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "predictions = gd_classifier.predict(X_test)\n",
    "results = gd_classifier.score(X_test, y_test)\n",
    "print(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VG Part\n",
    "\n",
    "To achieve a pass with distinction (VG) in this assignment, you must adequately solve the tasks above for a passing grade (G). In addition, you must:\n",
    "\n",
    "1. Implement the optimization as *stochastic* gradient descent (SGD)\n",
    "2. Implement a [tf-idf](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) feature model, and compare classification performance to bag-of-words (this should also be briefly discussed in your analysis). Choose your preferred formulation of tf-idf from the literature, *briefly* motivating your choice.\n",
    "3. Implement an extension to the SDG optimization of your choice, e.g. from [this list on wikipedia](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Extensions_and_variants).\n",
    "4. Implement [k-fold cross validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics)#k-fold_cross-validation) for evaluating and comparing your model variants.\n",
    "5. Prepare a presentation (~5min) with analysis of your design choices, pipelines, and results. How much did you gain in performance by using the more complex pipelines? The analysis and claims must be essentially correct. Submit and handful of slides in pdf format with your notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1.SGD \n",
    "1. Implement the optimization as *stochastic* gradient descent (SGD)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 2000\n",
      "Positive samples: 1000\n",
      "Negative samples: 1000\n",
      "     Metrics  Values\n",
      "0   Accuracy  0.8650\n",
      "1  Precision  0.8641\n",
      "2     Recall  0.8457\n",
      "3   F1_Score  0.8548\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "import os\n",
    "\n",
    "\n",
    "def parser(dataset_path='./txt_sentoken'):\n",
    "    X_raw, y = [], []\n",
    "    \n",
    "    pos_folder_path = os.path.join(dataset_path, 'pos')\n",
    "    for filename in os.listdir(pos_folder_path):\n",
    "        file_path = os.path.join(pos_folder_path, filename)\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\", errors='ignore') as file:\n",
    "            X_raw.append(file.read().strip().lower())\n",
    "        y.append(1)\n",
    "    \n",
    "\n",
    "    neg_folder_path = os.path.join(dataset_path, 'neg')\n",
    "    for filename in os.listdir(neg_folder_path):\n",
    "        file_path = os.path.join(neg_folder_path, filename)\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\", errors='ignore') as file:\n",
    "            X_raw.append(file.read().strip().lower())\n",
    "        y.append(-1)\n",
    "    \n",
    "    y = np.array(y)\n",
    "\n",
    "    print(f\"Total samples: {len(X_raw)}\")\n",
    "    print(f\"Positive samples: {sum(y == 1)}\")\n",
    "    print(f\"Negative samples: {sum(y == -1)}\")\n",
    "\n",
    "    return X_raw, y\n",
    "\n",
    "class CountVectorizer:\n",
    "    def __init__(self):\n",
    "        self.vocabulary_ = None\n",
    "        self.ordered_vocabulary = None\n",
    "    \n",
    "    def fit(self, X_raw):\n",
    "        words_set = set()\n",
    "        for text in X_raw:\n",
    "            words_set.update(text.lower().split())\n",
    "        self.ordered_vocabulary = sorted(words_set)\n",
    "        self.vocabulary_ = {}\n",
    "        for i, word in enumerate(self.ordered_vocabulary):\n",
    "            self.vocabulary_[word] = i\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X_raw):\n",
    "        rows = []\n",
    "        cols = []\n",
    "        data = []\n",
    "        for i, text in enumerate(X_raw):\n",
    "            for word in set(text.lower().split()):\n",
    "                if word in self.vocabulary_:\n",
    "                    rows.append(i)\n",
    "                    cols.append(self.vocabulary_[word])\n",
    "                    data.append(1)\n",
    "        X = sparse.csr_matrix((data, (rows, cols)), shape=(len(X_raw), len(self.vocabulary_)))\n",
    "        return X\n",
    "    \n",
    "    def fit_transform(self, X_raw):\n",
    "        self.fit(X_raw)\n",
    "        return self.transform(X_raw)\n",
    "    \n",
    "    def get_feature_names_out(self):\n",
    "        return self.ordered_vocabulary\n",
    "\n",
    "class SGDClassifier:\n",
    "    def __init__(self, learning_rate=0.001, reguliser_dampening=0.01, max_iterations=1000, tolerance=1e-5):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.reguliser_dampening = reguliser_dampening\n",
    "        self.max_iterations = max_iterations\n",
    "        self.tolerance = tolerance\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "        self.loss_history = []\n",
    "    \n",
    "    def hinge_loss(self, X, y):\n",
    "        X_arr = X.toarray() if sparse.issparse(X) else X\n",
    "        scores = X_arr.dot(self.w) + self.b\n",
    "        margins = y * scores\n",
    "        reg_term = (self.reguliser_dampening / 2) * np.sum(self.w**2)\n",
    "        losses = np.maximum(0, 1 - margins)\n",
    "        hinge_term = np.mean(losses)\n",
    "        return reg_term + hinge_term\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        X_arr = X.toarray() if sparse.issparse(X) else X\n",
    "        self.w = np.zeros(n_features)\n",
    "        self.b = 0\n",
    "        \n",
    "        for epoch in range(self.max_iterations):\n",
    "            current_loss = self.hinge_loss(X_arr, y)\n",
    "            self.loss_history.append(current_loss)\n",
    "            \n",
    "            # Shuffle the data at the start of each epoch for stochastic updates\n",
    "            indices = np.random.permutation(n_samples)\n",
    "            for i in indices:\n",
    "                xi = X_arr[i, :]\n",
    "                yi = y[i]\n",
    "                \n",
    "                # Compute the margin for the current sample\n",
    "                margin = yi * (xi.dot(self.w) + self.b)\n",
    "                \n",
    "                # Compute the gradient for the current sample\n",
    "                grad_w = self.reguliser_dampening * self.w\n",
    "                grad_b = 0\n",
    "                \n",
    "                if margin < 1:  # Hinge loss condition\n",
    "                    grad_w -= yi * xi\n",
    "                    grad_b -= yi\n",
    "                \n",
    "                # Update the weights and bias based on the gradient of this single sample\n",
    "                self.w -= self.learning_rate * grad_w\n",
    "                self.b -= self.learning_rate * grad_b\n",
    "            \n",
    "            # Check for convergence based on loss change\n",
    "            if epoch > 0 and abs(self.loss_history[-1] - self.loss_history[-2]) < self.tolerance:\n",
    "                break\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X_arr = X.toarray() if sparse.issparse(X) else X\n",
    "        scores = X_arr.dot(self.w) + self.b\n",
    "        return np.sign(scores)\n",
    "\n",
    "    def score(self, X, y):\n",
    "        y_pred = self.predict(X)\n",
    "        tp = np.sum((y == 1) & (y_pred == 1))\n",
    "        fp = np.sum((y == -1) & (y_pred == 1))\n",
    "        tn = np.sum((y == -1) & (y_pred == -1))\n",
    "        fn = np.sum((y == 1) & (y_pred == -1))\n",
    "        accuracy = (tp + tn) / (tp + fp + tn + fn)\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        metrics = {\n",
    "            \"Metrics\": [\"Accuracy\", \"Precision\", \"Recall\", \"F1_Score\"],\n",
    "            \"Values\": [accuracy, precision, recall, f1]\n",
    "        }\n",
    "        df = pd.DataFrame(metrics)\n",
    "        df[\"Values\"] = df[\"Values\"].round(4)\n",
    "        return df\n",
    "\n",
    "\n",
    "def manual_train_test_split(X, y, train_size=0.8):\n",
    "    n_samples = X.shape[0] # Get the number of samples\n",
    "    train_samples = int(n_samples * train_size) # Calculate the number of training samples\n",
    "    indices = np.arange(n_samples) # Create an array of indices\n",
    "    np.random.shuffle(indices)# Shuffle the indices\n",
    "    train_indices = indices[:train_samples]# Select the training indices\n",
    "    test_indices = indices[train_samples:]# Select the test indices\n",
    "    return X[train_indices], X[test_indices], y[train_indices], y[test_indices]\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    X_raw, y = parser()\n",
    "    vectorizer = CountVectorizer()  \n",
    "    vectorizer.fit(X_raw)\n",
    "    X = vectorizer.transform(X_raw)\n",
    "    X_train, X_test, y_train, y_test = manual_train_test_split(X, y)\n",
    "    gd_classifier = SGDClassifier(learning_rate=0.01, reguliser_dampening=0.001, max_iterations=1000)\n",
    "    gd_classifier.fit(X_train, y_train)\n",
    "    predictions = gd_classifier.predict(X_test)\n",
    "    results = gd_classifier.score(X_test, y_test)\n",
    "    print(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    \n",
    "## 2.Tf-idf\n",
    "Implement a [tf-idf](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) feature model, and compare classification performance to bag-of-words (this should also be briefly discussed in your analysis). Choose your preferred formulation of tf-idf from the literature, *briefly* motivating your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 2000\n",
      "Positive samples: 1000\n",
      "Negative samples: 1000\n",
      "     Metrics  Values\n",
      "0   Accuracy  0.8675\n",
      "1  Precision  0.8889\n",
      "2     Recall  0.8502\n",
      "3   F1_Score  0.8691\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "import os\n",
    "\n",
    "\n",
    "def parser(dataset_path='./txt_sentoken'):\n",
    "    X_raw, y = [], []\n",
    "    \n",
    "    pos_folder_path = os.path.join(dataset_path, 'pos')\n",
    "    for filename in os.listdir(pos_folder_path):\n",
    "        file_path = os.path.join(pos_folder_path, filename)\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\", errors='ignore') as file:\n",
    "            X_raw.append(file.read().strip().lower())\n",
    "        y.append(1)\n",
    "    \n",
    "\n",
    "    neg_folder_path = os.path.join(dataset_path, 'neg')\n",
    "    for filename in os.listdir(neg_folder_path):\n",
    "        file_path = os.path.join(neg_folder_path, filename)\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\", errors='ignore') as file:\n",
    "            X_raw.append(file.read().strip().lower())\n",
    "        y.append(-1)\n",
    "    \n",
    "    y = np.array(y)\n",
    "\n",
    "    print(f\"Total samples: {len(X_raw)}\")\n",
    "    print(f\"Positive samples: {sum(y == 1)}\")\n",
    "    print(f\"Negative samples: {sum(y == -1)}\")\n",
    "\n",
    "    return X_raw, y\n",
    "\n",
    "class TfidfVectorizer:\n",
    "    def __init__(self, sparse_output=True):\n",
    "        self.vocabulary = None\n",
    "        self.ordered_vocabulary = None\n",
    "        self.sparse_output = sparse_output\n",
    "        self.idf = None\n",
    "    \n",
    "    def fit(self, X_raw):\n",
    "        doc_count = {}\n",
    "        total_docs = len(X_raw)\n",
    "        words_set = set()\n",
    "        for text in X_raw:\n",
    "            words = set(text.split())\n",
    "            words_set.update(words)\n",
    "            for word in words:\n",
    "                if word in doc_count:\n",
    "                    doc_count[word] += 1\n",
    "                else:\n",
    "                    doc_count[word] = 1\n",
    "        self.ordered_vocabulary = sorted(words_set)\n",
    "        self.vocabulary = {}\n",
    "        self.idf = {}\n",
    "        for i, word in enumerate(self.ordered_vocabulary):\n",
    "            self.vocabulary[word] = i\n",
    "            self.idf[word] = np.log(total_docs/doc_count[word])\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X_raw):\n",
    "        rows = []\n",
    "        cols = []\n",
    "        data = []\n",
    "        for i, text in enumerate(X_raw):\n",
    "            word_counts = {}\n",
    "            for word in text.split():\n",
    "                if word in self.vocabulary:\n",
    "                    if word in word_counts:\n",
    "                        word_counts[word] += 1\n",
    "                    else:\n",
    "                        word_counts[word] = 1\n",
    "            for word, count in word_counts.items():\n",
    "                rows.append(i)\n",
    "                cols.append(self.vocabulary[word])\n",
    "                data.append(count*self.idf[word])\n",
    "        X = sparse.csr_matrix((data, (rows, cols)), shape=(len(X_raw), len(self.vocabulary)))\n",
    "        if not self.sparse_output:\n",
    "            X = X.toarray()\n",
    "        return X\n",
    "    \n",
    "    def fit_transform(self, X_raw):\n",
    "        self.fit(X_raw)\n",
    "        return self.transform(X_raw)\n",
    "    \n",
    "    def get_feature_names_out(self):\n",
    "        return self.ordered_vocabulary\n",
    "\n",
    "class SGDClassifier:\n",
    "    def __init__(self, learning_rate=0.001, reguliser_dampening=0.01, max_iterations=1000, tolerance=1e-5):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.reguliser_dampening = reguliser_dampening\n",
    "        self.max_iterations = max_iterations\n",
    "        self.tolerance = tolerance\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "        self.loss_history = []\n",
    "    \n",
    "    def hinge_loss(self, X, y):\n",
    "        X_arr = X.toarray() if sparse.issparse(X) else X\n",
    "        scores = X_arr.dot(self.w) + self.b\n",
    "        margins = y * scores\n",
    "        reg_term = (self.reguliser_dampening / 2) * np.sum(self.w**2)\n",
    "        losses = np.maximum(0, 1 - margins)\n",
    "        hinge_term = np.mean(losses)\n",
    "        return reg_term + hinge_term\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        X_arr = X.toarray() if sparse.issparse(X) else X\n",
    "        self.w = np.zeros(n_features)\n",
    "        self.b = 0\n",
    "        \n",
    "        for epoch in range(self.max_iterations):\n",
    "            current_loss = self.hinge_loss(X_arr, y)\n",
    "            self.loss_history.append(current_loss)\n",
    "            \n",
    "            # Shuffle the data at the start of each epoch for stochastic updates\n",
    "            indices = np.random.permutation(n_samples)\n",
    "            for i in indices:\n",
    "                xi = X_arr[i, :]\n",
    "                yi = y[i]\n",
    "                \n",
    "                # Compute the margin for the current sample\n",
    "                margin = yi * (xi.dot(self.w) + self.b)\n",
    "                \n",
    "                # Compute the gradient for the current sample\n",
    "                grad_w = self.reguliser_dampening * self.w\n",
    "                grad_b = 0\n",
    "                \n",
    "                if margin < 1:  # Hinge loss condition\n",
    "                    grad_w -= yi * xi\n",
    "                    grad_b -= yi\n",
    "                \n",
    "                # Update the weights and bias based on the gradient of this single sample\n",
    "                self.w -= self.learning_rate * grad_w\n",
    "                self.b -= self.learning_rate * grad_b\n",
    "            \n",
    "            # Check for convergence based on loss change\n",
    "            if epoch > 0 and abs(self.loss_history[-1] - self.loss_history[-2]) < self.tolerance:\n",
    "                break\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X_arr = X.toarray() if sparse.issparse(X) else X\n",
    "        scores = X_arr.dot(self.w) + self.b\n",
    "        return np.sign(scores)\n",
    "\n",
    "    def score(self, X, y):\n",
    "        y_pred = self.predict(X)\n",
    "        tp = np.sum((y == 1) & (y_pred == 1))\n",
    "        fp = np.sum((y == -1) & (y_pred == 1))\n",
    "        tn = np.sum((y == -1) & (y_pred == -1))\n",
    "        fn = np.sum((y == 1) & (y_pred == -1))\n",
    "        accuracy = (tp + tn) / (tp + fp + tn + fn)\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        metrics = {\n",
    "            \"Metrics\": [\"Accuracy\", \"Precision\", \"Recall\", \"F1_Score\"],\n",
    "            \"Values\": [accuracy, precision, recall, f1]\n",
    "        }\n",
    "        df = pd.DataFrame(metrics)\n",
    "        df[\"Values\"] = df[\"Values\"].round(4)\n",
    "        return df\n",
    "\n",
    "\n",
    "def manual_train_test_split(X, y, train_size=0.8):\n",
    "    n_samples = X.shape[0] # Get the number of samples\n",
    "    train_samples = int(n_samples * train_size) # Calculate the number of training samples\n",
    "    indices = np.arange(n_samples) # Create an array of indices\n",
    "    np.random.shuffle(indices)# Shuffle the indices\n",
    "    train_indices = indices[:train_samples]# Select the training indices\n",
    "    test_indices = indices[train_samples:]# Select the test indices\n",
    "    return X[train_indices], X[test_indices], y[train_indices], y[test_indices]\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    X_raw, y = parser()\n",
    "    vectorizer = TfidfVectorizer()  \n",
    "    vectorizer.fit(X_raw)\n",
    "    X = vectorizer.transform(X_raw)\n",
    "    X_train, X_test, y_train, y_test = manual_train_test_split(X, y)\n",
    "    gd_classifier = SGDClassifier(learning_rate=0.001, reguliser_dampening=0.001, max_iterations=1000)\n",
    "    gd_classifier.fit(X_train, y_train)\n",
    "    predictions = gd_classifier.predict(X_test)\n",
    "    results = gd_classifier.score(X_test, y_test)\n",
    "    print(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.adam     \n",
    "3. Implement an extension to the SDG optimization of your choice, e.g. from [this list on wikipedia](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Extensions_and_variants)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 2000\n",
      "Positive samples: 1000\n",
      "Negative samples: 1000\n",
      "Starting training for 1000 epochs\n",
      "Features: 5000, Samples: 1600, Batch size: 32\n",
      "Final loss: 0.000163\n",
      "     Metrics  Values\n",
      "0   Accuracy  0.8400\n",
      "1  Precision  0.7921\n",
      "2     Recall  0.8791\n",
      "3   F1_Score  0.8333\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "import os\n",
    "\n",
    "\n",
    "def parser(dataset_path='./txt_sentoken'):\n",
    "    X_raw, y = [], []\n",
    "    \n",
    "    pos_folder_path = os.path.join(dataset_path, 'pos')\n",
    "    for filename in os.listdir(pos_folder_path):\n",
    "        file_path = os.path.join(pos_folder_path, filename)\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\", errors='ignore') as file:\n",
    "            X_raw.append(file.read().strip().lower())\n",
    "        y.append(1)\n",
    "    \n",
    "    neg_folder_path = os.path.join(dataset_path, 'neg')\n",
    "    for filename in os.listdir(neg_folder_path):\n",
    "        file_path = os.path.join(neg_folder_path, filename)\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\", errors='ignore') as file:\n",
    "            X_raw.append(file.read().strip().lower())\n",
    "        y.append(-1)\n",
    "    \n",
    "    y = np.array(y)\n",
    "\n",
    "    print(f\"Total samples: {len(X_raw)}\")\n",
    "    print(f\"Positive samples: {sum(y == 1)}\")\n",
    "    print(f\"Negative samples: {sum(y == -1)}\")\n",
    "\n",
    "    return X_raw, y\n",
    "\n",
    "\n",
    "class TfidfVectorizer:\n",
    "    def __init__(self, sparse_output=True, max_features=None):\n",
    "        self.vocabulary = None\n",
    "        self.ordered_vocabulary = None\n",
    "        self.sparse_output = sparse_output\n",
    "        self.idf = None\n",
    "        self.max_features = max_features\n",
    "    \n",
    "    def fit(self, X_raw):\n",
    "        doc_count = {}\n",
    "        total_docs = len(X_raw)\n",
    "        words_set = set()\n",
    "        \n",
    "        for text in X_raw:\n",
    "            words = set(text.split())\n",
    "            words_set.update(words)\n",
    "            for word in words:\n",
    "                doc_count[word] = doc_count.get(word, 0) + 1\n",
    "        \n",
    "        if self.max_features and len(words_set) > self.max_features:\n",
    "            sorted_words = sorted(words_set, key=lambda w: doc_count[w], reverse=True)\n",
    "            words_set = set(sorted_words[:self.max_features])\n",
    "        \n",
    "        self.ordered_vocabulary = sorted(words_set)\n",
    "        self.vocabulary = {word: i for i, word in enumerate(self.ordered_vocabulary)}\n",
    "        self.idf = {word: np.log(total_docs/doc_count[word]) for word in self.vocabulary}\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X_raw):\n",
    "        rows, cols, data = [], [], []\n",
    "        \n",
    "        for i, text in enumerate(X_raw):\n",
    "            word_counts = {}\n",
    "            words = text.split()\n",
    "            \n",
    "            for word in words:\n",
    "                if word in self.vocabulary:\n",
    "                    word_counts[word] = word_counts.get(word, 0) + 1\n",
    "            \n",
    "            for word, count in word_counts.items():\n",
    "                rows.append(i)\n",
    "                cols.append(self.vocabulary[word])\n",
    "                data.append(count * self.idf[word])\n",
    "        \n",
    "        X = sparse.csr_matrix((data, (rows, cols)), shape=(len(X_raw), len(self.vocabulary)))\n",
    "        \n",
    "        if not self.sparse_output:\n",
    "            X = X.toarray()\n",
    "        \n",
    "        return X\n",
    "    \n",
    "    def fit_transform(self, X_raw):\n",
    "        self.fit(X_raw)\n",
    "        return self.transform(X_raw)\n",
    "    \n",
    "    def get_feature_names_out(self):\n",
    "        return self.ordered_vocabulary\n",
    "\n",
    "\n",
    "class SGDClassifier:\n",
    "    def __init__(self, learning_rate=0.01, reguliser_dampening=0.0001, max_iterations=100, \n",
    "                 tolerance=1e-4, batch_size=32, use_adam=True):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.reguliser_dampening = reguliser_dampening\n",
    "        self.max_iterations = max_iterations\n",
    "        self.tolerance = tolerance\n",
    "        self.batch_size = batch_size\n",
    "        self.use_adam = use_adam\n",
    "        self.w = None\n",
    "        self.b = 0\n",
    "        self.loss_history = []\n",
    "        self.beta1 = 0.9\n",
    "        self.beta2 = 0.999\n",
    "        self.epsilon = 1e-8\n",
    "    \n",
    "    def hinge_loss(self, X, y, sample_indices=None):\n",
    "        if sample_indices is None:\n",
    "            n_samples = X.shape[0]\n",
    "            sample_size = min(200, n_samples)\n",
    "            sample_indices = np.random.choice(n_samples, sample_size, replace=False)\n",
    "        \n",
    "        X_sample = X[sample_indices]\n",
    "        y_sample = y[sample_indices]\n",
    "        \n",
    "        scores = X_sample.dot(self.w) + self.b\n",
    "        \n",
    "        margins = y_sample * scores\n",
    "        reg_term = (self.reguliser_dampening / 2) * np.sum(self.w**2)\n",
    "        hinge_term = np.mean(np.maximum(0, 1 - margins))\n",
    "        \n",
    "        return reg_term + hinge_term\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.w = np.zeros(n_features)\n",
    "        self.b = 0\n",
    "        \n",
    "        if self.use_adam:\n",
    "            m_w = np.zeros(n_features)\n",
    "            v_w = np.zeros(n_features)\n",
    "            m_b = 0\n",
    "            v_b = 0\n",
    "            t = 0\n",
    "        \n",
    "        loss_check_interval = max(1, min(10, self.max_iterations // 10))\n",
    "        prev_loss = float('inf')\n",
    "        \n",
    "        print(f\"Starting training for {self.max_iterations} epochs\")\n",
    "        print(f\"Features: {n_features}, Samples: {n_samples}, Batch size: {self.batch_size}\")\n",
    "        \n",
    "        for epoch in range(self.max_iterations):\n",
    "            if epoch % loss_check_interval == 0:\n",
    "                current_loss = self.hinge_loss(X, y)\n",
    "                self.loss_history.append(current_loss)\n",
    "                #print(f\"Epoch {epoch}, Loss: {current_loss:.6f}\")\n",
    "                \n",
    "                if epoch > 0 and abs(current_loss - prev_loss) < self.tolerance:\n",
    "                    #print(f\"Converged at epoch {epoch}\")\n",
    "                    break\n",
    "                prev_loss = current_loss\n",
    "            \n",
    "            indices = np.random.permutation(n_samples)\n",
    "            \n",
    "            for start_idx in range(0, n_samples, self.batch_size):\n",
    "                end_idx = min(start_idx + self.batch_size, n_samples)\n",
    "                batch_indices = indices[start_idx:end_idx]\n",
    "                \n",
    "                if self.use_adam:\n",
    "                    t += 1\n",
    "                \n",
    "                X_batch = X[batch_indices]\n",
    "                y_batch = y[batch_indices]\n",
    "                \n",
    "                scores = X_batch.dot(self.w) + self.b\n",
    "                margins = y_batch * scores\n",
    "                \n",
    "                violated_indices = margins < 1\n",
    "                grad_w = self.reguliser_dampening * self.w\n",
    "                \n",
    "                if sparse.issparse(X_batch):\n",
    "                    X_violated = X_batch[violated_indices]\n",
    "                    y_violated = y_batch[violated_indices]\n",
    "                    \n",
    "                    if X_violated.shape[0] > 0:\n",
    "                        grad_w -= (X_violated.multiply(y_violated.reshape(-1, 1))).sum(axis=0).A1 / batch_indices.size\n",
    "                        grad_b = -np.sum(y_violated) / batch_indices.size\n",
    "                    else:\n",
    "                        grad_b = 0\n",
    "                else:\n",
    "                    if np.any(violated_indices):\n",
    "                        grad_w -= np.dot(X_batch[violated_indices].T, y_batch[violated_indices]) / batch_indices.size\n",
    "                        grad_b = -np.sum(y_batch[violated_indices]) / batch_indices.size\n",
    "                    else:\n",
    "                        grad_b = 0\n",
    "                \n",
    "                if self.use_adam:\n",
    "                    m_w = self.beta1 * m_w + (1 - self.beta1) * grad_w\n",
    "                    m_b = self.beta1 * m_b + (1 - self.beta1) * grad_b\n",
    "                    \n",
    "                    v_w = self.beta2 * v_w + (1 - self.beta2) * (grad_w * grad_w)\n",
    "                    v_b = self.beta2 * v_b + (1 - self.beta2) * (grad_b * grad_b)\n",
    "                    \n",
    "                    m_w_hat = m_w / (1 - self.beta1**t)\n",
    "                    m_b_hat = m_b / (1 - self.beta1**t)\n",
    "                    \n",
    "                    v_w_hat = v_w / (1 - self.beta2**t)\n",
    "                    v_b_hat = v_b / (1 - self.beta2**t)\n",
    "                    \n",
    "                    self.w -= self.learning_rate * m_w_hat / (np.sqrt(v_w_hat) + self.epsilon)\n",
    "                    self.b -= self.learning_rate * m_b_hat / (np.sqrt(v_b_hat) + self.epsilon)\n",
    "                else:\n",
    "                    self.w -= self.learning_rate * grad_w\n",
    "                    self.b -= self.learning_rate * grad_b\n",
    "        \n",
    "        final_loss = self.hinge_loss(X, y)\n",
    "        self.loss_history.append(final_loss)\n",
    "        print(f\"Final loss: {final_loss:.6f}\")\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        scores = X.dot(self.w) + self.b\n",
    "        return np.sign(scores)\n",
    "\n",
    "    def score(self, X, y):\n",
    "        y_pred = self.predict(X)\n",
    "        tp = np.sum((y == 1) & (y_pred == 1))\n",
    "        fp = np.sum((y == -1) & (y_pred == 1))\n",
    "        tn = np.sum((y == -1) & (y_pred == -1))\n",
    "        fn = np.sum((y == 1) & (y_pred == -1))\n",
    "        \n",
    "        accuracy = (tp + tn) / (tp + fp + tn + fn)\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        metrics = {\n",
    "            \"Metrics\": [\"Accuracy\", \"Precision\", \"Recall\", \"F1_Score\"],\n",
    "            \"Values\": [accuracy, precision, recall, f1]\n",
    "        }\n",
    "        df = pd.DataFrame(metrics)\n",
    "        df[\"Values\"] = df[\"Values\"].round(4)\n",
    "        return df\n",
    "\n",
    "\n",
    "def manual_train_test_split(X, y, train_size=0.8, random_state=None):\n",
    "    if random_state is not None:\n",
    "        np.random.seed(random_state)\n",
    "        \n",
    "    n_samples = X.shape[0]\n",
    "    train_samples = int(n_samples * train_size)\n",
    "    indices = np.arange(n_samples)\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    train_indices = indices[:train_samples]\n",
    "    test_indices = indices[train_samples:]\n",
    "    \n",
    "    X_train = X[train_indices]\n",
    "    X_test = X[test_indices]\n",
    "    y_train = y[train_indices]\n",
    "    y_test = y[test_indices]\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    X_raw, y = parser()\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(max_features=5000)  \n",
    "    X = vectorizer.fit_transform(X_raw)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = manual_train_test_split(X, y, random_state=42)\n",
    "    \n",
    "    sgd_classifier = SGDClassifier(\n",
    "        learning_rate=0.001,\n",
    "        reguliser_dampening=0.0001,\n",
    "        max_iterations=1000,\n",
    "        batch_size=32,\n",
    "        use_adam=True\n",
    "    )\n",
    "    \n",
    "    sgd_classifier.fit(X_train, y_train)\n",
    "    results = sgd_classifier.score(X_test, y_test)\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 4.k-fold cross validation\n",
    "1. Implement [k-fold cross validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics)#k-fold_cross-validation) for evaluating and comparing your model variants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 2000\n",
      "Positive samples: 1000\n",
      "Negative samples: 1000\n",
      "     Metrics  Values\n",
      "0   Accuracy  0.8225\n",
      "1  Precision  0.8286\n",
      "2     Recall  0.8325\n",
      "3   F1_Score  0.8305\n",
      "\n",
      "===== Running 5-fold cross-validation =====\n",
      "Processing fold 1/5\n",
      "Fold 1 results:\n",
      "     Metrics  Values\n",
      "0   Accuracy  0.8275\n",
      "1  Precision  0.8502\n",
      "2     Recall  0.8224\n",
      "3   F1_Score  0.8361\n",
      "Processing fold 2/5\n",
      "Fold 2 results:\n",
      "     Metrics  Values\n",
      "0   Accuracy  0.8375\n",
      "1  Precision  0.8148\n",
      "2     Recall  0.8756\n",
      "3   F1_Score  0.8441\n",
      "Processing fold 3/5\n",
      "Fold 3 results:\n",
      "     Metrics  Values\n",
      "0   Accuracy  0.8225\n",
      "1  Precision  0.8128\n",
      "2     Recall  0.8333\n",
      "3   F1_Score  0.8229\n",
      "Processing fold 4/5\n",
      "Fold 4 results:\n",
      "     Metrics  Values\n",
      "0   Accuracy  0.8250\n",
      "1  Precision  0.8238\n",
      "2     Recall  0.8154\n",
      "3   F1_Score  0.8196\n",
      "Processing fold 5/5\n",
      "Fold 5 results:\n",
      "     Metrics  Values\n",
      "0   Accuracy  0.8150\n",
      "1  Precision  0.7980\n",
      "2     Recall  0.8229\n",
      "3   F1_Score  0.8103\n",
      "\n",
      "===== Cross-validation Summary =====\n",
      "     Metrics    Mean     Std\n",
      "0   Accuracy  0.8255  0.0073\n",
      "1  Precision  0.8199  0.0173\n",
      "2     Recall  0.8339  0.0216\n",
      "3   F1_Score  0.8266  0.0120\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "import os\n",
    "\n",
    "\n",
    "def parser(dataset_path='./txt_sentoken'):\n",
    "    X_raw, y = [], []\n",
    "    \n",
    "    pos_folder_path = os.path.join(dataset_path, 'pos')\n",
    "    for filename in os.listdir(pos_folder_path):\n",
    "        file_path = os.path.join(pos_folder_path, filename)\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\", errors='ignore') as file:\n",
    "            X_raw.append(file.read().strip().lower())\n",
    "        y.append(1)\n",
    "    \n",
    "\n",
    "    neg_folder_path = os.path.join(dataset_path, 'neg')\n",
    "    for filename in os.listdir(neg_folder_path):\n",
    "        file_path = os.path.join(neg_folder_path, filename)\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\", errors='ignore') as file:\n",
    "            X_raw.append(file.read().strip().lower())\n",
    "        y.append(-1)\n",
    "    \n",
    "    y = np.array(y)\n",
    "\n",
    "    print(f\"Total samples: {len(X_raw)}\")\n",
    "    print(f\"Positive samples: {sum(y == 1)}\")\n",
    "    print(f\"Negative samples: {sum(y == -1)}\")\n",
    "\n",
    "    return X_raw, y\n",
    "\n",
    "class TfidfVectorizer:\n",
    "    def __init__(self, sparse_output=True):\n",
    "        self.vocabulary = None\n",
    "        self.ordered_vocabulary = None\n",
    "        self.sparse_output = sparse_output\n",
    "        self.idf = None\n",
    "    \n",
    "    def fit(self, X_raw):\n",
    "        doc_count = {}\n",
    "        total_docs = len(X_raw)\n",
    "        words_set = set()\n",
    "        for text in X_raw:\n",
    "            words = set(text.split())\n",
    "            words_set.update(words)\n",
    "            for word in words:\n",
    "                if word in doc_count:\n",
    "                    doc_count[word] += 1\n",
    "                else:\n",
    "                    doc_count[word] = 1\n",
    "        self.ordered_vocabulary = sorted(words_set)\n",
    "        self.vocabulary = {}\n",
    "        self.idf = {}\n",
    "        for i, word in enumerate(self.ordered_vocabulary):\n",
    "            self.vocabulary[word] = i\n",
    "            self.idf[word] = np.log(total_docs/doc_count[word])\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X_raw):\n",
    "        rows = []\n",
    "        cols = []\n",
    "        data = []\n",
    "        for i, text in enumerate(X_raw):\n",
    "            word_counts = {}\n",
    "            for word in text.split():\n",
    "                if word in self.vocabulary:\n",
    "                    if word in word_counts:\n",
    "                        word_counts[word] += 1\n",
    "                    else:\n",
    "                        word_counts[word] = 1\n",
    "            for word, count in word_counts.items():\n",
    "                rows.append(i)\n",
    "                cols.append(self.vocabulary[word])\n",
    "                data.append(count*self.idf[word])\n",
    "        X = sparse.csr_matrix((data, (rows, cols)), shape=(len(X_raw), len(self.vocabulary)))\n",
    "        if not self.sparse_output:\n",
    "            X = X.toarray()\n",
    "        return X\n",
    "    \n",
    "    def fit_transform(self, X_raw):\n",
    "        self.fit(X_raw)\n",
    "        return self.transform(X_raw)\n",
    "    \n",
    "    def get_feature_names_out(self):\n",
    "        return self.ordered_vocabulary\n",
    "\n",
    "class SGDClassifier:\n",
    "    def __init__(self, learning_rate=0.001, reguliser_dampening=0.01, max_iterations=1000, tolerance=1e-5):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.reguliser_dampening = reguliser_dampening\n",
    "        self.max_iterations = max_iterations\n",
    "        self.tolerance = tolerance\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "        self.loss_history = []\n",
    "    \n",
    "    def hinge_loss(self, X, y):\n",
    "        X_arr = X.toarray() if sparse.issparse(X) else X\n",
    "        scores = X_arr.dot(self.w) + self.b\n",
    "        margins = y * scores\n",
    "        reg_term = (self.reguliser_dampening / 2) * np.sum(self.w**2)\n",
    "        losses = np.maximum(0, 1 - margins)\n",
    "        hinge_term = np.mean(losses)\n",
    "        return reg_term + hinge_term\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        X_arr = X.toarray() if sparse.issparse(X) else X\n",
    "        self.w = np.zeros(n_features)\n",
    "        self.b = 0\n",
    "        \n",
    "        for epoch in range(self.max_iterations):\n",
    "            current_loss = self.hinge_loss(X_arr, y)\n",
    "            self.loss_history.append(current_loss)\n",
    "            \n",
    "            # Shuffle the data at the start of each epoch for stochastic updates\n",
    "            indices = np.random.permutation(n_samples)\n",
    "            for i in indices:\n",
    "                xi = X_arr[i, :]\n",
    "                yi = y[i]\n",
    "                \n",
    "                # Compute the margin for the current sample\n",
    "                margin = yi * (xi.dot(self.w) + self.b)\n",
    "                \n",
    "                # Compute the gradient for the current sample\n",
    "                grad_w = self.reguliser_dampening * self.w\n",
    "                grad_b = 0\n",
    "                \n",
    "                if margin < 1:  # Hinge loss condition\n",
    "                    grad_w -= yi * xi\n",
    "                    grad_b -= yi\n",
    "                \n",
    "                # Update the weights and bias based on the gradient of this single sample\n",
    "                self.w -= self.learning_rate * grad_w\n",
    "                self.b -= self.learning_rate * grad_b\n",
    "            \n",
    "            # Check for convergence based on loss change\n",
    "            if epoch > 0 and abs(self.loss_history[-1] - self.loss_history[-2]) < self.tolerance:\n",
    "                break\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X_arr = X.toarray() if sparse.issparse(X) else X\n",
    "        scores = X_arr.dot(self.w) + self.b\n",
    "        return np.sign(scores)\n",
    "\n",
    "    def score(self, X, y):\n",
    "        y_pred = self.predict(X)\n",
    "        tp = np.sum((y == 1) & (y_pred == 1))\n",
    "        fp = np.sum((y == -1) & (y_pred == 1))\n",
    "        tn = np.sum((y == -1) & (y_pred == -1))\n",
    "        fn = np.sum((y == 1) & (y_pred == -1))\n",
    "        accuracy = (tp + tn) / (tp + fp + tn + fn)\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        metrics = {\n",
    "            \"Metrics\": [\"Accuracy\", \"Precision\", \"Recall\", \"F1_Score\"],\n",
    "            \"Values\": [accuracy, precision, recall, f1]\n",
    "        }\n",
    "        df = pd.DataFrame(metrics)\n",
    "        df[\"Values\"] = df[\"Values\"].round(4)\n",
    "        return df\n",
    "\n",
    "\n",
    "def manual_train_test_split(X, y, train_size=0.8):\n",
    "    n_samples = X.shape[0] # Get the number of samples\n",
    "    train_samples = int(n_samples * train_size) # Calculate the number of training samples\n",
    "    indices = np.arange(n_samples) # Create an array of indices\n",
    "    np.random.shuffle(indices)# Shuffle the indices\n",
    "    train_indices = indices[:train_samples]# Select the training indices\n",
    "    test_indices = indices[train_samples:]# Select the test indices\n",
    "    return X[train_indices], X[test_indices], y[train_indices], y[test_indices]\n",
    "\n",
    "\n",
    "def k_fold_cross_validation(X, y, k=5, model_params=None):\n",
    "    if model_params is None:\n",
    "        model_params = {\n",
    "            'learning_rate': 0.001,\n",
    "            'reguliser_dampening': 0.001,\n",
    "            'max_iterations': 1000\n",
    "        }\n",
    "    \n",
    "    n_samples = X.shape[0]\n",
    "    fold_size = n_samples // k\n",
    "    indices = np.arange(n_samples)\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    all_metrics = []\n",
    "    \n",
    "    for fold in range(k):\n",
    "        print(f\"Processing fold {fold+1}/{k}\")\n",
    "        \n",
    "        test_start = fold * fold_size\n",
    "        test_end = test_start + fold_size if fold < k - 1 else n_samples\n",
    "        test_indices = indices[test_start:test_end]\n",
    "        train_indices = np.concatenate([indices[:test_start], indices[test_end:]])\n",
    "        \n",
    "        X_train, X_test = X[train_indices], X[test_indices]\n",
    "        y_train, y_test = y[train_indices], y[test_indices]\n",
    "        \n",
    "        model = SGDClassifier(**model_params)\n",
    "        model.fit(X_train, y_train)\n",
    "        fold_metrics = model.score(X_test, y_test)\n",
    "        print(f\"Fold {fold+1} results:\")\n",
    "        print(fold_metrics)\n",
    "        \n",
    "        all_metrics.append(fold_metrics[\"Values\"].values)\n",
    "    \n",
    "    avg_values = np.mean(all_metrics, axis=0)\n",
    "    std_values = np.std(all_metrics, axis=0)\n",
    "    \n",
    "    summary = pd.DataFrame({\n",
    "        \"Metrics\": [\"Accuracy\", \"Precision\", \"Recall\", \"F1_Score\"],\n",
    "        \"Mean\": avg_values.round(4),\n",
    "        \"Std\": std_values.round(4)\n",
    "    })\n",
    "    \n",
    "    return summary\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    X_raw, y = parser()\n",
    "    vectorizer = TfidfVectorizer()  \n",
    "    vectorizer.fit(X_raw)\n",
    "    X = vectorizer.transform(X_raw)\n",
    "    X_train, X_test, y_train, y_test = manual_train_test_split(X, y)\n",
    "    gd_classifier = SGDClassifier(learning_rate=0.001, reguliser_dampening=0.001, max_iterations=1000)\n",
    "    gd_classifier.fit(X_train, y_train)\n",
    "    predictions = gd_classifier.predict(X_test)\n",
    "    results = gd_classifier.score(X_test, y_test)\n",
    "    print(results)\n",
    "    \n",
    "    # Add k-fold cross-validation\n",
    "    print(\"\\n===== Running 5-fold cross-validation =====\")\n",
    "    # Convert to array for easier indexing in k-fold\n",
    "    X_array = X.toarray() if sparse.issparse(X) else X\n",
    "    cv_results = k_fold_cross_validation(X_array, y, k=5)\n",
    "    print(\"\\n===== Cross-validation Summary =====\")\n",
    "    print(cv_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
